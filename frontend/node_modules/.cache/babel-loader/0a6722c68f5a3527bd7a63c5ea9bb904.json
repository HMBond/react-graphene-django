{"ast":null,"code":"import { stringifyVariables as e, createRequest as t, formatDocument as r, makeOperation as i } from \"@urql/core\";\nimport { share as n, merge as a, mergeMap as o, fromArray as s, buffer as u, fromPromise as c, skipUntil as l, map as d, filter as f, empty as p, makeSubject as v } from \"wonka\";\nimport { _ as y } from \"./5301ccd2.mjs\";\nimport { Kind as m } from \"graphql/language/kinds.mjs\";\nimport { valueFromASTUntyped as h } from \"graphql/utilities/valueFromASTUntyped.mjs\";\nimport { print as g } from \"graphql/language/printer.mjs\";\n\nfunction getName(e) {\n  return e.name.value;\n}\n\nfunction getFragmentTypeName(e) {\n  return e.typeCondition.name.value;\n}\n\nfunction getFieldAlias(e) {\n  return e.alias ? e.alias.value : getName(e);\n}\n\nfunction getSelectionSet(e) {\n  return e.selectionSet ? e.selectionSet.selections : [];\n}\n\nfunction getTypeCondition(e) {\n  return e.typeCondition ? getName(e.typeCondition) : null;\n}\n\nfunction isFieldNode(e) {\n  return e.kind === m.FIELD;\n}\n\nfunction isInlineFragment(e) {\n  return e.kind === m.INLINE_FRAGMENT;\n}\n\nfunction getFieldArguments(e, t) {\n  var r = {};\n  var i = 0;\n\n  if (e.arguments && e.arguments.length) {\n    for (var n = 0, a = e.arguments.length; n < a; n++) {\n      var o = e.arguments[n];\n      var s = h(o.value, t);\n\n      if (null != s) {\n        r[getName(o)] = s;\n        i++;\n      }\n    }\n  }\n\n  return i > 0 ? r : null;\n}\n\nfunction filterVariables(e, t) {\n  if (!t || !e.variableDefinitions) {\n    return;\n  }\n\n  var r = {};\n\n  for (var i = 0, n = e.variableDefinitions.length; i < n; i++) {\n    var a = getName(e.variableDefinitions[i].variable);\n    r[a] = t[a];\n  }\n\n  return r;\n}\n\nfunction normalizeVariables(e, t) {\n  var r = {};\n\n  if (!t) {\n    return r;\n  }\n\n  if (e.variableDefinitions) {\n    for (var i = 0, n = e.variableDefinitions.length; i < n; i++) {\n      var a = e.variableDefinitions[i];\n      var o = getName(a.variable);\n      r[o] = void 0 === t[o] && a.defaultValue ? h(a.defaultValue, t) : t[o];\n    }\n  }\n\n  for (var s in t) {\n    if (!(s in r)) {\n      r[s] = t[s];\n    }\n  }\n\n  return r;\n}\n\nvar N = \"\\nhttps://bit.ly/2XbVrpR#\";\nvar k = new Set();\nvar O = [];\n\nfunction popDebugNode() {\n  return O.pop();\n}\n\nfunction pushDebugNode(e, t) {\n  var r = \"\";\n\n  if (t.kind === m.INLINE_FRAGMENT) {\n    r = e ? 'Inline Fragment on \"' + e + '\"' : \"Inline Fragment\";\n  } else if (t.kind === m.OPERATION_DEFINITION) {\n    r = (t.name ? '\"' + t.name.value + '\"' : \"Unnamed\") + \" \" + t.operation;\n  } else if (t.kind === m.FRAGMENT_DEFINITION) {\n    r = '\"' + t.name.value + '\" Fragment';\n  }\n\n  if (r) {\n    O.push(r);\n  }\n}\n\nfunction getDebugOutput() {\n  return O.length ? \"\\n(Caused At: \" + O.join(\", \") + \")\" : \"\";\n}\n\nfunction invariant(e, t, r) {\n  if (!e) {\n    var i = t || \"Minfied Error #\" + r + \"\\n\";\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      i += getDebugOutput();\n    }\n\n    var n = new Error(i + N + r);\n    n.name = \"Graphcache Error\";\n    throw n;\n  }\n}\n\nfunction warn(e, t) {\n  if (!k.has(e)) {\n    console.warn(e + getDebugOutput() + N + t);\n    k.add(e);\n  }\n}\n\nfunction getMainOperation(e) {\n  for (var t = 0; t < e.definitions.length; t++) {\n    if (e.definitions[t].kind === m.OPERATION_DEFINITION) {\n      return e.definitions[t];\n    }\n  }\n\n  invariant(!1, \"production\" !== process.env.NODE_ENV ? \"Invalid GraphQL document: All GraphQL documents must contain an OperationDefinitionnode for a query, subscription, or mutation.\" : \"\", 1);\n}\n\nfunction getFragments(e) {\n  var t = {};\n\n  for (var r = 0; r < e.definitions.length; r++) {\n    var i = e.definitions[r];\n\n    if (i.kind === m.FRAGMENT_DEFINITION) {\n      t[getName(i)] = i;\n    }\n  }\n\n  return t;\n}\n\nfunction shouldInclude(e, t) {\n  for (var r = 0; e.directives && r < e.directives.length; r++) {\n    var i = e.directives[r];\n    var n = getName(i);\n\n    if ((\"include\" === n || \"skip\" === n) && i.arguments && i.arguments[0] && \"if\" === getName(i.arguments[0])) {\n      var a = h(i.arguments[0].value, t);\n      return \"include\" === n ? !!a : !a;\n    }\n  }\n\n  return !0;\n}\n\nfunction isDeferred(e, t) {\n  for (var r = 0; e.directives && r < e.directives.length; r++) {\n    var i = e.directives[r];\n\n    if (\"defer\" === getName(i)) {\n      for (var n = 0; i.arguments && n < i.arguments.length; n++) {\n        var a = i.arguments[r];\n\n        if (\"if\" === getName(a)) {\n          return !!h(a.value, t);\n        }\n      }\n\n      return !0;\n    }\n  }\n\n  return !1;\n}\n\nfunction isFieldNullable(e, t, r) {\n  var i = getField(e, t, r);\n  return !!i && \"NON_NULL\" !== i.type.kind;\n}\n\nfunction isListNullable(e, t, r) {\n  var i = getField(e, t, r);\n\n  if (!i) {\n    return !1;\n  }\n\n  var n = \"NON_NULL\" === i.type.kind ? i.type.ofType : i.type;\n  return \"LIST\" === n.kind && \"NON_NULL\" !== n.ofType.kind;\n}\n\nfunction isFieldAvailableOnType(e, t, r) {\n  return 0 === r.indexOf(\"__\") || 0 === t.indexOf(\"__\") || !!getField(e, t, r);\n}\n\nfunction isInterfaceOfType(e, t, r) {\n  if (!r) {\n    return !1;\n  }\n\n  var i = getTypeCondition(t);\n\n  if (!i || r === i) {\n    return !0;\n  } else if (e.types[i] && \"OBJECT\" === e.types[i].kind) {\n    return i === r;\n  }\n\n  !function expectAbstractType(e, t) {\n    invariant(e.types[t] && (\"INTERFACE\" === e.types[t].kind || \"UNION\" === e.types[t].kind), \"production\" !== process.env.NODE_ENV ? \"Invalid Abstract type: The type `\" + t + \"` is not an Interface or Union type in the defined schema, but a fragment in the GraphQL document is using it as a type condition.\" : \"\", 5);\n  }(e, i);\n  expectObjectType(e, r);\n  return e.isSubType(i, r);\n}\n\nfunction getField(e, t, r) {\n  if (0 === r.indexOf(\"__\") || 0 === t.indexOf(\"__\")) {\n    return;\n  }\n\n  expectObjectType(e, t);\n  var i = e.types[t].fields[r];\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    if (!i) {\n      warn(\"Invalid field: The field `\" + r + \"` does not exist on `\" + t + \"`, but the GraphQL document expects it to exist.\\nTraversal will continue, however this may lead to undefined behavior!\", 4);\n    }\n  }\n\n  return i;\n}\n\nfunction expectObjectType(e, t) {\n  invariant(e.types[t] && \"OBJECT\" === e.types[t].kind, \"production\" !== process.env.NODE_ENV ? \"Invalid Object type: The type `\" + t + \"` is not an object in the defined schema, but the GraphQL document is traversing it.\" : \"\", 3);\n}\n\nfunction warnAboutResolver(e) {\n  \"production\" !== process.env.NODE_ENV && warn(\"Invalid resolver: `\" + e + \"` is not in the defined schema, but the `resolvers` option is referencing it.\", 23);\n}\n\nfunction keyOfField(t, r) {\n  return r ? t + \"(\" + e(r) + \")\" : t;\n}\n\nfunction joinKeys(e, t) {\n  return e + \".\" + t;\n}\n\nfunction fieldInfoOfKey(e) {\n  var t = e.indexOf(\"(\");\n\n  if (t > -1) {\n    return {\n      fieldKey: e,\n      fieldName: e.slice(0, t),\n      arguments: JSON.parse(e.slice(t + 1, -1))\n    };\n  } else {\n    return {\n      fieldKey: e,\n      fieldName: e,\n      arguments: null\n    };\n  }\n}\n\nfunction deserializeKeyInfo(e) {\n  var t = e.indexOf(\".\");\n  return {\n    entityKey: e.slice(0, t).replace(/%2e/g, \".\"),\n    fieldKey: e.slice(t + 1)\n  };\n}\n\nfunction makeDict() {\n  return Object.create(null);\n}\n\nvar _ = null;\nvar E = null;\nvar b = null;\nvar D = null;\nvar w = null;\nvar F = null;\nvar S = !1;\n\nfunction makeNodeMap() {\n  return {\n    optimistic: makeDict(),\n    base: new Map()\n  };\n}\n\nfunction makeData(e) {\n  var t;\n\n  if (e) {\n    if (_.has(e)) {\n      return e;\n    }\n\n    t = E.get(e) || y({}, e);\n    E.set(e, t);\n  } else {\n    t = {};\n  }\n\n  _.add(t);\n\n  return t;\n}\n\nfunction ownsData(e) {\n  return !!e && _.has(e);\n}\n\nfunction initDataState(e, t, r, i) {\n  _ = new Set();\n  E = new Map();\n  b = e;\n  D = t;\n  w = makeDict();\n  S = !!i;\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    O.length = 0;\n  }\n\n  if (!r) {\n    F = null;\n  } else if (i || t.optimisticOrder.length > 0) {\n    if (!i && !t.commutativeKeys.has(r)) {\n      reserveLayer(t, r);\n    } else if (i) {\n      t.commutativeKeys.delete(r);\n    }\n\n    F = r;\n    !function createLayer(e, t) {\n      if (-1 === e.optimisticOrder.indexOf(t)) {\n        e.optimisticOrder.unshift(t);\n      }\n\n      if (!e.refLock[t]) {\n        e.refLock[t] = makeDict();\n        e.links.optimistic[t] = new Map();\n        e.records.optimistic[t] = new Map();\n      }\n    }(t, r);\n  } else {\n    F = null;\n    deleteLayer(t, r);\n  }\n}\n\nfunction clearDataState() {\n  if (\"production\" !== process.env.NODE_ENV) {\n    getCurrentDependencies();\n  }\n\n  var t = D;\n  var r = F;\n  S = !1;\n  F = null;\n\n  if (r && t.optimisticOrder.indexOf(r) > -1) {\n    var i = t.optimisticOrder.length;\n\n    while (--i >= 0 && t.refLock[t.optimisticOrder[i]] && t.commutativeKeys.has(t.optimisticOrder[i]) && !t.deferredKeys.has(t.optimisticOrder[i])) {\n      squashLayer(t.optimisticOrder[i]);\n    }\n  }\n\n  _ = null;\n  E = null;\n  b = null;\n  D = null;\n  w = null;\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    O.length = 0;\n  }\n\n  if (\"test\" !== process.env.NODE_ENV && !t.defer) {\n    t.defer = !0;\n    Promise.resolve().then(function () {\n      initDataState(\"read\", t, null);\n      !function gc() {\n        D.gc.forEach(function (e, t, r) {\n          if ((D.refCount[e] || 0) > 0) {\n            r.delete(e);\n            return;\n          }\n\n          for (var i in D.refLock) {\n            var n = D.refLock[i];\n\n            if ((n[e] || 0) > 0) {\n              return;\n            }\n\n            delete n[e];\n          }\n\n          delete D.refCount[e];\n          r.delete(e);\n          D.records.base.delete(e);\n          var a = D.links.base.get(e);\n\n          if (a) {\n            D.links.base.delete(e);\n\n            for (var o in a) {\n              updateRCForLink(r, D.refCount, a[o], -1);\n            }\n          }\n        });\n      }();\n      !function persistData() {\n        if (D.storage) {\n          S = !0;\n          b = \"read\";\n          var t = makeDict();\n          D.persist.forEach(function (r) {\n            var i = deserializeKeyInfo(r);\n            var n = i.entityKey;\n            var a = i.fieldKey;\n            var o;\n\n            if (void 0 !== (o = readLink(n, a))) {\n              t[r] = \":\" + e(o);\n            } else if (void 0 !== (o = readRecord(n, a))) {\n              t[r] = e(o);\n            } else {\n              t[r] = void 0;\n            }\n          });\n          S = !1;\n          D.storage.writeData(t);\n          D.persist.clear();\n        }\n      }();\n      clearDataState();\n      t.defer = !1;\n    });\n  }\n}\n\nfunction noopDataState(e, t, r) {\n  if (t && !r) {\n    e.deferredKeys.delete(t);\n  }\n\n  initDataState(\"read\", e, t, r);\n  clearDataState();\n}\n\nfunction getCurrentOperation() {\n  invariant(null !== b, \"production\" !== process.env.NODE_ENV ? \"Invalid Cache call: The cache may only be accessed or mutated duringoperations like write or query, or as part of its resolvers, updaters, or optimistic configs.\" : \"\", 2);\n  return b;\n}\n\nfunction getCurrentDependencies() {\n  invariant(null !== w, \"production\" !== process.env.NODE_ENV ? \"Invalid Cache call: The cache may only be accessed or mutated duringoperations like write or query, or as part of its resolvers, updaters, or optimistic configs.\" : \"\", 2);\n  return w;\n}\n\nfunction setNode(e, t, r, i) {\n  var n = F ? e.optimistic[F] : e.base;\n  var a = n.get(t);\n\n  if (void 0 === a) {\n    n.set(t, a = makeDict());\n  }\n\n  if (void 0 === i && !F) {\n    delete a[r];\n  } else {\n    a[r] = i;\n  }\n}\n\nfunction getNode(e, t, r) {\n  var i;\n  var n = !S && \"read\" === b && F && D.commutativeKeys.has(F);\n\n  for (var a = 0, o = D.optimisticOrder.length; a < o; a++) {\n    var s = D.optimisticOrder[a];\n    var u = e.optimistic[s];\n    n = n && s !== F;\n\n    if (u && (!n || !D.commutativeKeys.has(s)) && (!S || \"write\" === b || D.commutativeKeys.has(s)) && void 0 !== (i = u.get(t)) && r in i) {\n      return i[r];\n    }\n  }\n\n  return void 0 !== (i = e.base.get(t)) ? i[r] : void 0;\n}\n\nfunction updateRCForEntity(e, t, r, i) {\n  var n = void 0 !== t[r] ? t[r] : 0;\n  var a = t[r] = n + i | 0;\n\n  if (void 0 !== e) {\n    if (a <= 0) {\n      e.add(r);\n    } else if (n <= 0 && a > 0) {\n      e.delete(r);\n    }\n  }\n}\n\nfunction updateRCForLink(e, t, r, i) {\n  if (\"string\" == typeof r) {\n    updateRCForEntity(e, t, r, i);\n  } else if (Array.isArray(r)) {\n    for (var n = 0, a = r.length; n < a; n++) {\n      if (Array.isArray(r[n])) {\n        updateRCForLink(e, t, r[n], i);\n      } else if (r[n]) {\n        updateRCForEntity(e, t, r[n], i);\n      }\n    }\n  }\n}\n\nfunction extractNodeFields(e, t, r) {\n  if (void 0 !== r) {\n    for (var i in r) {\n      if (!t.has(i)) {\n        e.push(fieldInfoOfKey(i));\n        t.add(i);\n      }\n    }\n  }\n}\n\nfunction extractNodeMapFields(e, t, r, i) {\n  extractNodeFields(e, t, i.base.get(r));\n\n  for (var n = 0, a = D.optimisticOrder.length; n < a; n++) {\n    var o = i.optimistic[D.optimisticOrder[n]];\n\n    if (void 0 !== o) {\n      extractNodeFields(e, t, o.get(r));\n    }\n  }\n}\n\nfunction updateDependencies(e, t) {\n  if (\"__typename\" !== t) {\n    if (e !== D.queryRootKey) {\n      w[e] = !0;\n    } else if (void 0 !== t) {\n      w[joinKeys(e, t)] = !0;\n    }\n  }\n}\n\nfunction updatePersist(e, t) {\n  if (!S && D.storage) {\n    D.persist.add(function serializeKeys(e, t) {\n      return e.replace(/\\./g, \"%2e\") + \".\" + t;\n    }(e, t));\n  }\n}\n\nfunction readRecord(e, t) {\n  updateDependencies(e, t);\n  return getNode(D.records, e, t);\n}\n\nfunction readLink(e, t) {\n  updateDependencies(e, t);\n  return getNode(D.links, e, t);\n}\n\nfunction writeRecord(e, t, r) {\n  updateDependencies(e, t);\n  updatePersist(e, t);\n  setNode(D.records, e, t, r);\n}\n\nfunction writeLink(e, t, r) {\n  var i = D;\n  var n;\n  var a;\n  var o;\n\n  if (F) {\n    n = i.refLock[F] || (i.refLock[F] = makeDict());\n    a = i.links.optimistic[F];\n  } else {\n    n = i.refCount;\n    a = i.links.base;\n    o = i.gc;\n  }\n\n  var s = a && a.get(e);\n  var u = s && s[t];\n  updateDependencies(e, t);\n  updatePersist(e, t);\n  setNode(i.links, e, t, r);\n  updateRCForLink(o, n, u, -1);\n  updateRCForLink(o, n, r, 1);\n}\n\nfunction reserveLayer(e, t, r) {\n  var i = e.optimisticOrder.indexOf(t);\n\n  if (-1 === i) {\n    e.optimisticOrder.unshift(t);\n  } else if (!e.commutativeKeys.has(t)) {\n    clearLayer(e, t);\n    e.optimisticOrder.splice(i, 1);\n    e.optimisticOrder.unshift(t);\n  }\n\n  if (r) {\n    e.deferredKeys.add(t);\n  } else {\n    e.deferredKeys.delete(t);\n  }\n\n  e.commutativeKeys.add(t);\n}\n\nfunction clearLayer(e, t) {\n  if (e.refLock[t]) {\n    delete e.refLock[t];\n    delete e.records.optimistic[t];\n    delete e.links.optimistic[t];\n    e.deferredKeys.delete(t);\n  }\n}\n\nfunction deleteLayer(e, t) {\n  var r = e.optimisticOrder.indexOf(t);\n\n  if (r > -1) {\n    e.optimisticOrder.splice(r, 1);\n    e.commutativeKeys.delete(t);\n  }\n\n  clearLayer(e, t);\n}\n\nfunction squashLayer(e) {\n  var t = w;\n  w = makeDict();\n  var r = D.links.optimistic[e];\n\n  if (r) {\n    r.forEach(function (e, t) {\n      for (var r in e) {\n        writeLink(t, r, e[r]);\n      }\n    });\n  }\n\n  var i = D.records.optimistic[e];\n\n  if (i) {\n    i.forEach(function (e, t) {\n      for (var r in e) {\n        writeRecord(t, r, e[r]);\n      }\n    });\n  }\n\n  w = t;\n  deleteLayer(D, e);\n}\n\nfunction inspectFields(e) {\n  var t = D.links;\n  var r = D.records;\n  var i = [];\n  var n = new Set();\n  updateDependencies(e);\n  extractNodeMapFields(i, n, e, t);\n  extractNodeMapFields(i, n, e, r);\n  return i;\n}\n\nvar L = {\n  current: null\n};\nvar x = {\n  current: !1\n};\n\nfunction getFieldError(e) {\n  return e.__internal.path.length > 0 && e.__internal.errorMap ? e.__internal.errorMap[e.__internal.path.join(\".\")] : void 0;\n}\n\nfunction makeContext(e, t, r, i, n, a, o) {\n  var s = {\n    store: e,\n    variables: t,\n    fragments: r,\n    parent: {\n      __typename: i\n    },\n    parentTypeName: i,\n    parentKey: n,\n    parentFieldKey: \"\",\n    fieldName: \"\",\n    error: void 0,\n    partial: !1,\n    optimistic: !!a,\n    __internal: {\n      path: [],\n      errorMap: void 0\n    }\n  };\n\n  if (o && o.graphQLErrors) {\n    for (var u = 0; u < o.graphQLErrors.length; u++) {\n      var c = o.graphQLErrors[u];\n\n      if (c.path && c.path.length) {\n        if (!s.__internal.errorMap) {\n          s.__internal.errorMap = Object.create(null);\n        }\n\n        s.__internal.errorMap[c.path.join(\".\")] = c;\n      }\n    }\n  }\n\n  return s;\n}\n\nfunction updateContext(e, t, r, i, n, a) {\n  L.current = e;\n  e.parent = t;\n  e.parentTypeName = r;\n  e.parentKey = i;\n  e.parentFieldKey = n;\n  e.fieldName = a;\n  e.error = getFieldError(e);\n}\n\nfunction isFragmentHeuristicallyMatching(e, t, r, i) {\n  if (!t) {\n    return !1;\n  }\n\n  var n = getTypeCondition(e);\n\n  if (!n || t === n) {\n    return !0;\n  }\n\n  \"production\" !== process.env.NODE_ENV && warn(\"Heuristic Fragment Matching: A fragment is trying to match against the `\" + t + \"` type, but the type condition is `\" + n + \"`. Since GraphQL allows for interfaces `\" + n + \"` may be aninterface.\\nA schema needs to be defined for this match to be deterministic, otherwise the fragment will be matched heuristically!\", 16);\n  return !getSelectionSet(e).some(function (e) {\n    if (!isFieldNode(e)) {\n      return !1;\n    }\n\n    var t = keyOfField(getName(e), getFieldArguments(e, i));\n    return !function hasField(e, t) {\n      return void 0 !== readRecord(e, t) || void 0 !== readLink(e, t);\n    }(r, t);\n  });\n}\n\nfunction makeSelectionIterator(e, t, r, i) {\n  var n = !1;\n  var a;\n  var o = 0;\n  return function next() {\n    if (!x.current && n) {\n      x.current = n;\n    }\n\n    if (a) {\n      var s = a();\n\n      if (null != s) {\n        return s;\n      }\n\n      a = void 0;\n      n = !1;\n\n      if (\"production\" !== process.env.NODE_ENV) {\n        popDebugNode();\n      }\n    }\n\n    while (o < r.length) {\n      var u = r[o++];\n\n      if (!shouldInclude(u, i.variables)) {\n        continue;\n      } else if (!isFieldNode(u)) {\n        var c = !isInlineFragment(u) ? i.fragments[getName(u)] : u;\n\n        if (void 0 !== c) {\n          if (i.store.schema ? isInterfaceOfType(i.store.schema, c, e) : isFragmentHeuristicallyMatching(c, e, t, i.variables)) {\n            if (\"production\" !== process.env.NODE_ENV) {\n              pushDebugNode(e, c);\n            }\n\n            n = !!isDeferred(u, i.variables);\n\n            if (!x.current && n) {\n              x.current = n;\n            }\n\n            return (a = makeSelectionIterator(e, t, getSelectionSet(c), i))();\n          }\n        }\n      } else {\n        return u;\n      }\n    }\n  };\n}\n\nfunction ensureData(e) {\n  return null == e ? null : e;\n}\n\nfunction ensureLink(e, t) {\n  if (null == t) {\n    return t;\n  } else if (Array.isArray(t)) {\n    var r = new Array(t.length);\n\n    for (var i = 0, n = r.length; i < n; i++) {\n      r[i] = ensureLink(e, t[i]);\n    }\n\n    return r;\n  }\n\n  var a = e.keyOfEntity(t);\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    if (!a && t && \"object\" == typeof t) {\n      warn(\"Can't generate a key for link(...) item.\\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `\" + t.__typename + \"`.\", 12);\n    }\n  }\n\n  return a;\n}\n\nfunction write(e, t, r, i, n) {\n  initDataState(\"write\", e.data, n || null);\n  var a = startWrite(e, t, r, i);\n  clearDataState();\n  return a;\n}\n\nfunction startWrite(e, t, r, i, n) {\n  var a = getMainOperation(t.query);\n  var o = {\n    data: r,\n    dependencies: getCurrentDependencies()\n  };\n  var s = e.rootFields[a.operation];\n  var u = makeContext(e, normalizeVariables(a, t.variables), getFragments(t.query), s, s, !!n, i);\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    pushDebugNode(s, a);\n  }\n\n  writeSelection(u, s, getSelectionSet(a), r);\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    popDebugNode();\n  }\n\n  return o;\n}\n\nfunction writeSelection(e, t, r, i) {\n  var n = t === e.store.rootFields.query;\n  var a = !n && !!e.store.rootNames[t];\n  var o = a || n ? t : i.__typename;\n\n  if (!o) {\n    \"production\" !== process.env.NODE_ENV && warn(\"Couldn't find __typename when writing.\\nIf you're writing to the cache manually have to pass a `__typename` property on each entity in your data.\", 14);\n    return;\n  } else if (!a && !n && t) {\n    writeRecord(t, \"__typename\", o);\n  }\n\n  var s = makeSelectionIterator(o, t || o, r, e);\n  var u;\n\n  while (u = s()) {\n    var c = getName(u);\n    var l = getFieldArguments(u, e.variables);\n    var d = keyOfField(c, l);\n    var f = getFieldAlias(u);\n    var p = i[f];\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      if (!a && void 0 === p && !x.current) {\n        \"production\" !== process.env.NODE_ENV && warn(\"Invalid undefined: The field at `\" + d + \"` is `undefined`, but the GraphQL query expects a \" + (void 0 === u.selectionSet ? \"scalar (number, boolean, etc)\" : \"selection set\") + \" for this field.\" + (e.optimistic ? \"\\nYour optimistic result may be missing a field!\" : \"\"), 13);\n        continue;\n      } else if (e.store.schema && o && \"__typename\" !== c) {\n        isFieldAvailableOnType(e.store.schema, o, c);\n      }\n    }\n\n    if (\"__typename\" === c || void 0 === p && x.current) {\n      continue;\n    }\n\n    e.__internal.path.push(f);\n\n    if (e.optimistic && a) {\n      var v = e.store.optimisticMutations[c];\n\n      if (!v) {\n        continue;\n      }\n\n      updateContext(e, i, o, o, d, c);\n      p = i[f] = ensureData(v(l || {}, e.store, e));\n    }\n\n    if (u.selectionSet) {\n      if (t && !a) {\n        var y = joinKeys(t, d);\n        writeLink(t || o, d, writeField(e, getSelectionSet(u), ensureData(p), y));\n      } else {\n        writeField(e, getSelectionSet(u), ensureData(p));\n      }\n    } else if (t && !a) {\n      writeRecord(t || o, d, null !== p || !getFieldError(e) ? p : void 0);\n    }\n\n    if (a) {\n      var m = e.store.updates[o][c];\n\n      if (m) {\n        updateContext(e, i, o, o, joinKeys(o, d), c);\n        i[c] = p;\n        m(i, l || {}, e.store, e);\n      }\n    }\n\n    e.__internal.path.pop();\n  }\n}\n\nvar q = /^__|PageInfo|(Connection|Edge)$/;\n\nfunction writeField(e, t, r, i) {\n  if (Array.isArray(r)) {\n    var n = new Array(r.length);\n\n    for (var a = 0, o = r.length; a < o; a++) {\n      e.__internal.path.push(a);\n\n      var s = i ? joinKeys(i, \"\" + a) : void 0;\n      var u = writeField(e, t, r[a], s);\n      n[a] = u;\n\n      e.__internal.path.pop();\n    }\n\n    return n;\n  } else if (null === r) {\n    return getFieldError(e) ? void 0 : null;\n  }\n\n  var c = e.store.keyOfEntity(r);\n  var l = r.__typename;\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    if (i && !e.store.keys[r.__typename] && null === c && \"string\" == typeof l && !q.test(l)) {\n      warn(\"Invalid key: The GraphQL query at the field at `\" + i + \"` has a selection set, but no key could be generated for the data at this field.\\nYou have to request `id` or `_id` fields for all selection sets or create a custom `keys` config for `\" + l + \"`.\\nEntities without keys will be embedded directly on the parent entity. If this is intentional, create a `keys` config for `\" + l + \"` that always returns null.\", 15);\n    }\n  }\n\n  var d = c || i;\n  writeSelection(e, d, t, r);\n  return d || null;\n}\n\nfunction Store(e) {\n  var t, r;\n  this.keyOfField = keyOfField;\n  this.resolveFieldByKey = this.resolve;\n\n  if (!e) {\n    e = {};\n  }\n\n  this.resolvers = e.resolvers || {};\n  this.optimisticMutations = e.optimistic || {};\n  this.keys = e.keys || {};\n  var i = \"Query\";\n  var n = \"Mutation\";\n  var a = \"Subscription\";\n\n  if (e.schema) {\n    var o = function buildClientSchema(e) {\n      var t = e.__schema;\n      var r = {};\n\n      function buildNameMap(e) {\n        var t = {};\n\n        for (var r = 0; r < e.length; r++) {\n          t[e[r].name] = e[r];\n        }\n\n        return t;\n      }\n\n      function buildType(e) {\n        switch (e.kind) {\n          case \"OBJECT\":\n          case \"INTERFACE\":\n            return {\n              name: e.name,\n              kind: e.kind,\n              interfaces: buildNameMap(e.interfaces || []),\n              fields: buildNameMap(e.fields.map(function (e) {\n                return {\n                  name: e.name,\n                  type: e.type,\n                  args: buildNameMap(e.args)\n                };\n              }))\n            };\n\n          case \"UNION\":\n            return {\n              name: e.name,\n              kind: e.kind,\n              types: buildNameMap(e.possibleTypes || [])\n            };\n        }\n      }\n\n      var i = {\n        query: t.queryType ? t.queryType.name : null,\n        mutation: t.mutationType ? t.mutationType.name : null,\n        subscription: t.subscriptionType ? t.subscriptionType.name : null,\n        types: void 0,\n        isSubType: function isSubType(e, t) {\n          var i = r[e];\n          var n = r[t];\n\n          if (!i || !n) {\n            return !1;\n          } else if (\"UNION\" === i.kind) {\n            return !!i.types[t];\n          } else if (\"OBJECT\" !== i.kind && \"OBJECT\" === n.kind) {\n            return !!n.interfaces[e];\n          } else {\n            return e === t;\n          }\n        }\n      };\n\n      if (t.types) {\n        i.types = r;\n\n        for (var n = 0; n < t.types.length; n++) {\n          var a = t.types[n];\n\n          if (a && a.name) {\n            var o = buildType(a);\n\n            if (o) {\n              r[a.name] = o;\n            }\n          }\n        }\n      }\n\n      return i;\n    }(e.schema);\n\n    i = o.query || i;\n    n = o.mutation || n;\n    a = o.subscription || a;\n\n    if (o.types) {\n      this.schema = o;\n    }\n  }\n\n  this.updates = ((t = {})[n] = e.updates && e.updates.Mutation || {}, t[a] = e.updates && e.updates.Subscription || {}, t);\n  this.rootFields = {\n    query: i,\n    mutation: n,\n    subscription: a\n  };\n  this.rootNames = ((r = {})[i] = \"query\", r[n] = \"mutation\", r[a] = \"subscription\", r);\n\n  this.data = function make(e) {\n    return {\n      defer: !1,\n      gc: new Set(),\n      persist: new Set(),\n      queryRootKey: e,\n      refCount: makeDict(),\n      refLock: makeDict(),\n      links: makeNodeMap(),\n      records: makeNodeMap(),\n      deferredKeys: new Set(),\n      commutativeKeys: new Set(),\n      optimisticOrder: [],\n      storage: null\n    };\n  }(i);\n\n  if (this.schema && \"production\" !== process.env.NODE_ENV) {\n    !function expectValidKeyingConfig(e, t) {\n      if (\"production\" !== process.env.NODE_ENV) {\n        for (var r in t) {\n          if (\"production\" !== process.env.NODE_ENV) {\n            if (!e.types[r]) {\n              warn(\"Invalid Object type: The type `\" + r + \"` is not an object in the defined schema, but the `keys` option is referencing it.\", 20);\n            }\n          }\n        }\n      }\n    }(this.schema, this.keys);\n    !function expectValidUpdatesConfig(e, t) {\n      if (\"production\" === process.env.NODE_ENV) {\n        return;\n      }\n\n      if (e.mutation) {\n        var r = e.types[e.mutation].fields;\n        var i = t[e.mutation] || {};\n\n        for (var n in i) {\n          if (\"production\" !== process.env.NODE_ENV) {\n            if (void 0 === r[n]) {\n              warn(\"Invalid mutation field: `\" + n + \"` is not in the defined schema, but the `updates.Mutation` option is referencing it.\", 21);\n            }\n          }\n        }\n      }\n\n      if (e.subscription) {\n        var a = e.types[e.subscription].fields;\n        var o = t[e.subscription] || {};\n\n        for (var s in o) {\n          if (\"production\" !== process.env.NODE_ENV) {\n            if (void 0 === a[s]) {\n              warn(\"Invalid subscription field: `\" + s + \"` is not in the defined schema, but the `updates.Subscription` option is referencing it.\", 22);\n            }\n          }\n        }\n      }\n    }(this.schema, this.updates);\n    !function expectValidResolversConfig(e, t) {\n      if (\"production\" === process.env.NODE_ENV) {\n        return;\n      }\n\n      for (var r in t) {\n        if (\"Query\" === r) {\n          if (e.query) {\n            var i = e.types[e.query].fields;\n\n            for (var n in t.Query) {\n              if (!i[n]) {\n                warnAboutResolver(\"Query.\" + n);\n              }\n            }\n          } else {\n            warnAboutResolver(\"Query\");\n          }\n        } else if (!e.types[r]) {\n          warnAboutResolver(r);\n        } else if (\"INTERFACE\" === e.types[r].kind || \"UNION\" === e.types[r].kind) {\n          s = r, u = e.types[r].kind, \"production\" !== process.env.NODE_ENV && warn(\"Invalid resolver: `\" + s + \"` does not match to a concrete type in the schema, but the `resolvers` option is referencing it. Implement the resolver for the types that \" + (\"UNION\" === u ? \"make up the union\" : \"implement the interface\") + \" instead.\", 26);\n        } else {\n          var a = e.types[r].fields;\n\n          for (var o in t[r]) {\n            if (!a[o]) {\n              warnAboutResolver(r + \".\" + o);\n            }\n          }\n        }\n      }\n\n      var s, u;\n    }(this.schema, this.resolvers);\n    !function expectValidOptimisticMutationsConfig(e, t) {\n      if (\"production\" === process.env.NODE_ENV) {\n        return;\n      }\n\n      if (e.mutation) {\n        var r = e.types[e.mutation].fields;\n\n        for (var i in t) {\n          if (\"production\" !== process.env.NODE_ENV) {\n            if (!r[i]) {\n              warn(\"Invalid optimistic mutation field: `\" + i + \"` is not a mutation field in the defined schema, but the `optimistic` option is referencing it.\", 24);\n            }\n          }\n        }\n      }\n    }(this.schema, this.optimisticMutations);\n  }\n}\n\nStore.prototype.keyOfEntity = function keyOfEntity(e) {\n  if (L.current && e === L.current.parent) {\n    return L.current.parentKey;\n  }\n\n  if (null == e || \"string\" == typeof e) {\n    return e || null;\n  }\n\n  if (!e.__typename) {\n    return null;\n  }\n\n  if (this.rootNames[e.__typename]) {\n    return e.__typename;\n  }\n\n  var t;\n\n  if (this.keys[e.__typename]) {\n    t = this.keys[e.__typename](e);\n  } else if (null != e.id) {\n    t = \"\" + e.id;\n  } else if (null != e._id) {\n    t = \"\" + e._id;\n  }\n\n  return t ? e.__typename + \":\" + t : null;\n};\n\nStore.prototype.resolve = function resolve(e, t, r) {\n  var i = keyOfField(t, r);\n  var n = this.keyOfEntity(e);\n\n  if (!n) {\n    return null;\n  }\n\n  var a = readRecord(n, i);\n\n  if (void 0 !== a) {\n    return a;\n  }\n\n  return readLink(n, i) || null;\n};\n\nStore.prototype.invalidate = function invalidate(e, t, r) {\n  var i = this.keyOfEntity(e);\n  invariant(i, \"production\" !== process.env.NODE_ENV ? \"Can't generate a key for invalidate(...).\\nYou have to pass an id or _id field or create a custom `keys` field for `\" + typeof e == \"object\" ? e.__typename : e + \"`.\" : \"\", 19);\n  !function invalidateEntity(e, t, r) {\n    var i = t ? [{\n      fieldKey: keyOfField(t, r)\n    }] : inspectFields(e);\n\n    for (var n = 0, a = i.length; n < a; n++) {\n      var o = i[n].fieldKey;\n\n      if (void 0 !== readLink(e, o)) {\n        writeLink(e, o, void 0);\n      } else {\n        writeRecord(e, o, void 0);\n      }\n    }\n  }(i, t, r);\n};\n\nStore.prototype.inspectFields = function inspectFields$1(e) {\n  var t = this.keyOfEntity(e);\n  return t ? inspectFields(t) : [];\n};\n\nStore.prototype.updateQuery = function updateQuery(e, i) {\n  var n = t(e.query, e.variables);\n  n.query = r(n.query);\n  var a = i(this.readQuery(n));\n\n  if (null !== a) {\n    startWrite(this, n, a);\n  }\n};\n\nStore.prototype.readQuery = function readQuery(e) {\n  var i = t(e.query, e.variables);\n  i.query = r(i.query);\n  return read(this, i).data;\n};\n\nStore.prototype.readFragment = function readFragment$1(e, t, i) {\n  return function readFragment(e, t, r, i) {\n    var n = getFragments(t);\n    var a = Object.keys(n);\n    var o = n[a[0]];\n\n    if (!o) {\n      \"production\" !== process.env.NODE_ENV && warn(\"readFragment(...) was called with an empty fragment.\\nYou have to call it with at least one fragment in your GraphQL document.\", 6);\n      return null;\n    }\n\n    var s = getFragmentTypeName(o);\n\n    if (\"string\" != typeof r && !r.__typename) {\n      r.__typename = s;\n    }\n\n    var u = e.keyOfEntity(r);\n\n    if (!u) {\n      \"production\" !== process.env.NODE_ENV && warn(\"Can't generate a key for readFragment(...).\\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `\" + s + \"`.\", 7);\n      return null;\n    }\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      pushDebugNode(s, o);\n    }\n\n    var c = readSelection(makeContext(e, i || {}, n, s, u), u, getSelectionSet(o), makeData()) || null;\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      popDebugNode();\n    }\n\n    return c;\n  }(this, r(e), t, i);\n};\n\nStore.prototype.writeFragment = function writeFragment$1(e, t, i) {\n  !function writeFragment(e, t, r, i) {\n    var n = getFragments(t);\n    var a = n[Object.keys(n)[0]];\n\n    if (!a) {\n      return \"production\" !== process.env.NODE_ENV ? warn(\"writeFragment(...) was called with an empty fragment.\\nYou have to call it with at least one fragment in your GraphQL document.\", 11) : void 0;\n    }\n\n    var o = getFragmentTypeName(a);\n    var s = y({}, {\n      __typename: o\n    }, r);\n    var u = e.keyOfEntity(s);\n\n    if (!u) {\n      return \"production\" !== process.env.NODE_ENV ? warn(\"Can't generate a key for writeFragment(...) data.\\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `\" + o + \"`.\", 12) : void 0;\n    }\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      pushDebugNode(o, a);\n    }\n\n    writeSelection(makeContext(e, i || {}, n, o, u, void 0), u, getSelectionSet(a), s);\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      popDebugNode();\n    }\n  }(this, r(e), t, i);\n};\n\nStore.prototype.link = function link(e, t, r, i) {\n  var n = void 0 !== i ? r : null;\n  var link = void 0 !== i ? i : r;\n  var a = ensureLink(this, e);\n\n  if (\"string\" == typeof a) {\n    writeLink(a, keyOfField(t, n), ensureLink(this, link));\n  }\n};\n\nfunction query(e, t, r, i, n) {\n  initDataState(\"read\", e.data, n);\n  var a = read(e, t, r, i);\n  clearDataState();\n  return a;\n}\n\nfunction read(e, t, r, i) {\n  var n = getMainOperation(t.query);\n  var a = e.rootFields[n.operation];\n  var o = getSelectionSet(n);\n  var s = makeContext(e, normalizeVariables(n, t.variables), getFragments(t.query), a, a, !1, i);\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    pushDebugNode(a, n);\n  }\n\n  if (!r) {\n    r = makeData();\n  }\n\n  var u = a !== s.store.rootFields.query ? readRoot(s, a, o, r) : readSelection(s, a, o, r);\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    popDebugNode();\n  }\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: s.partial || !u,\n    data: u || null\n  };\n}\n\nfunction readRoot(e, t, r, i) {\n  if (\"string\" != typeof (e.store.rootNames[t] ? t : i.__typename)) {\n    return i;\n  }\n\n  var n = makeSelectionIterator(t, t, r, e);\n  var a;\n  var o = !1;\n  var s = makeData(i);\n\n  while (a = n()) {\n    var u = getFieldAlias(a);\n    var c = i[u];\n\n    e.__internal.path.push(u);\n\n    var l = void 0;\n\n    if (a.selectionSet && null !== c) {\n      l = readRootField(e, getSelectionSet(a), ensureData(c));\n    } else {\n      l = c;\n    }\n\n    o = o || l !== c;\n\n    if (void 0 !== l) {\n      s[u] = l;\n    }\n\n    e.__internal.path.pop();\n  }\n\n  return o ? s : i;\n}\n\nfunction readRootField(e, t, r) {\n  if (Array.isArray(r)) {\n    var i = new Array(r.length);\n    var n = !1;\n\n    for (var a = 0, o = r.length; a < o; a++) {\n      e.__internal.path.push(a);\n\n      i[a] = readRootField(e, t, r[a]);\n      n = n || i[a] !== r[a];\n\n      e.__internal.path.pop();\n    }\n\n    return n ? i : r;\n  } else if (null === r) {\n    return null;\n  }\n\n  var s = e.store.keyOfEntity(r);\n\n  if (null !== s) {\n    return readSelection(e, s, t, r) || null;\n  } else {\n    return readRoot(e, r.__typename, t, r);\n  }\n}\n\nfunction readSelection(e, t, r, i, n) {\n  var a = e.store;\n  var o = t === a.rootFields.query;\n  var s = n && a.keyOfEntity(n) || t;\n\n  if (\"production\" !== process.env.NODE_ENV) {\n    if (!o && e.store.rootNames[s]) {\n      warn(\"Invalid root traversal: A selection was being read on `\" + s + \"` which is an uncached root type.\\nThe `\" + e.store.rootFields.mutation + \"` and `\" + e.store.rootFields.subscription + \"` types are special Operation Root Types and cannot be read back from the cache.\", 25);\n    }\n  }\n\n  var u = !o ? readRecord(s, \"__typename\") || n && n.__typename : t;\n\n  if (\"string\" != typeof u) {\n    return;\n  } else if (n && u !== n.__typename) {\n    \"production\" !== process.env.NODE_ENV && warn(\"Invalid resolver data: The resolver at `\" + s + \"` returned an invalid typename that could not be reconciled with the cache.\", 8);\n    return;\n  }\n\n  var c = makeSelectionIterator(u, s, r, e);\n  var l = !1;\n  var d = !1;\n  var f = u !== i.__typename;\n  var p;\n  var v = makeData(i);\n\n  while (void 0 !== (p = c())) {\n    var y = getName(p);\n    var m = getFieldArguments(p, e.variables);\n    var h = getFieldAlias(p);\n    var g = keyOfField(y, m);\n    var N = joinKeys(s, g);\n    var k = readRecord(s, g);\n    var O = n ? n[y] : void 0;\n    var _ = a.resolvers[u];\n\n    if (\"production\" !== process.env.NODE_ENV && a.schema && u) {\n      isFieldAvailableOnType(a.schema, u, y);\n    }\n\n    e.__internal.path.push(h);\n\n    var E = void 0;\n\n    if (\"__typename\" === y) {\n      E = u;\n    } else if (void 0 !== O && void 0 === p.selectionSet) {\n      E = O;\n    } else if (\"read\" === getCurrentOperation() && _ && \"function\" == typeof _[y]) {\n      updateContext(e, v, u, s, N, y);\n\n      if (void 0 !== k) {\n        v[h] = k;\n      }\n\n      E = _[y](v, m || {}, a, e);\n\n      if (p.selectionSet) {\n        E = resolveResolverResult(e, u, y, N, getSelectionSet(p), void 0 !== v[h] ? v[h] : i[h], E, ownsData(i));\n      }\n\n      if (a.schema && null === E && !isFieldNullable(a.schema, u, y)) {\n        return;\n      }\n    } else if (!p.selectionSet) {\n      E = k;\n    } else if (void 0 !== O) {\n      E = resolveResolverResult(e, u, y, N, getSelectionSet(p), void 0 !== v[h] ? v[h] : i[h], O, ownsData(i));\n    } else {\n      var b = readLink(s, g);\n\n      if (void 0 !== b) {\n        E = resolveLink(e, b, u, y, getSelectionSet(p), void 0 !== v[h] ? v[h] : i[h], ownsData(i));\n      } else if (\"object\" == typeof k && null !== k) {\n        E = k;\n      }\n    }\n\n    if (void 0 === E && x.current) {\n      l = !0;\n    } else if (void 0 === E && (a.schema && isFieldNullable(a.schema, u, y) || getFieldError(e))) {\n      d = !0;\n      E = null;\n    } else if (void 0 === E) {\n      e.__internal.path.pop();\n\n      return;\n    } else {\n      l = l || \"__typename\" !== y;\n    }\n\n    e.__internal.path.pop();\n\n    f = f || E !== i[h];\n\n    if (void 0 !== E) {\n      v[h] = E;\n    }\n  }\n\n  e.partial = e.partial || d;\n  return o && d && !l ? void 0 : f ? v : i;\n}\n\nfunction resolveResolverResult(e, t, r, i, n, a, o, s) {\n  if (Array.isArray(o)) {\n    var u = e.store;\n    var c = u.schema ? isListNullable(u.schema, t, r) : !1;\n    var l = new Array(o.length);\n    var d = !Array.isArray(a) || o.length !== a.length;\n\n    for (var f = 0, p = o.length; f < p; f++) {\n      e.__internal.path.push(f);\n\n      var v = resolveResolverResult(e, t, r, joinKeys(i, \"\" + f), n, null != a ? a[f] : void 0, o[f], s);\n\n      e.__internal.path.pop();\n\n      if (void 0 === v && !c) {\n        return;\n      } else {\n        e.partial = e.partial || void 0 === v && c;\n        l[f] = null != v ? v : null;\n        d = d || l[f] !== a[f];\n      }\n    }\n\n    return d ? l : a;\n  } else if (null == o) {\n    return o;\n  } else if (s && null === a) {\n    return null;\n  } else if (function isDataOrKey(e) {\n    return \"string\" == typeof e || \"object\" == typeof e && \"string\" == typeof e.__typename;\n  }(o)) {\n    var y = a || makeData();\n    return \"string\" == typeof o ? readSelection(e, o, n, y) : readSelection(e, i, n, y, o);\n  } else {\n    \"production\" !== process.env.NODE_ENV && warn(\"Invalid resolver value: The field at `\" + i + \"` is a scalar (number, boolean, etc), but the GraphQL query expects a selection set for this field.\", 9);\n    return;\n  }\n}\n\nfunction resolveLink(e, t, r, i, n, a, o) {\n  if (Array.isArray(t)) {\n    var s = e.store;\n    var u = s.schema ? isListNullable(s.schema, r, i) : !1;\n    var c = new Array(t.length);\n    var l = !Array.isArray(a) || c.length !== a.length;\n\n    for (var d = 0, f = t.length; d < f; d++) {\n      e.__internal.path.push(d);\n\n      var p = resolveLink(e, t[d], r, i, n, null != a ? a[d] : void 0, o);\n\n      e.__internal.path.pop();\n\n      if (void 0 === p && !u) {\n        return;\n      } else {\n        e.partial = e.partial || void 0 === p && u;\n        c[d] = p || null;\n        l = l || c[d] !== a[d];\n      }\n    }\n\n    return l ? c : a;\n  } else if (null === t || null === a && o) {\n    return null;\n  }\n\n  return readSelection(e, t, n, a || makeData());\n}\n\nfunction addCacheOutcome(e, t) {\n  return i(e.kind, e, y({}, e.context, {\n    meta: y({}, e.context.meta, {\n      cacheOutcome: t\n    })\n  }));\n}\n\nfunction toRequestPolicy(e, t) {\n  return i(e.kind, e, y({}, e.context, {\n    requestPolicy: t\n  }));\n}\n\nfunction cacheExchange(e) {\n  return function (t) {\n    var v = t.forward;\n    var y = t.client;\n    var m = t.dispatchDebug;\n    var h = new Store(e);\n    var g;\n\n    if (e && e.storage) {\n      g = e.storage.readData().then(function (t) {\n        !function hydrateData(e, t, r) {\n          initDataState(\"write\", e, null);\n\n          for (var i in r) {\n            var n = r[i];\n\n            if (void 0 !== n) {\n              var a = deserializeKeyInfo(i);\n              var o = a.entityKey;\n              var s = a.fieldKey;\n\n              if (\":\" === n[0]) {\n                writeLink(o, s, JSON.parse(n.slice(1)));\n              } else {\n                writeRecord(o, s, JSON.parse(n));\n              }\n            }\n          }\n\n          clearDataState();\n          e.storage = t;\n        }(h.data, e.storage, t);\n      });\n    }\n\n    var N = new Map();\n    var k = [];\n    var O = new Map();\n\n    var _ = new Map();\n\n    var E = makeDict();\n    var b = new Set();\n    var D = makeDict();\n\n    function isBlockedByOptimisticUpdate(e) {\n      for (var t in e) {\n        if (E[t]) {\n          return !0;\n        }\n      }\n\n      return !1;\n    }\n\n    function collectPendingOperations(e, t) {\n      if (t) {\n        for (var r in t) {\n          var i = D[r];\n\n          if (i) {\n            D[r] = [];\n\n            for (var n = 0, a = i.length; n < a; n++) {\n              e.add(i[n]);\n            }\n          }\n        }\n      }\n    }\n\n    function executePendingOperations(e, t) {\n      t.forEach(function (t) {\n        if (t !== e.key) {\n          var r = O.get(t);\n\n          if (r) {\n            O.delete(t);\n            var i = \"cache-first\";\n\n            if (b.has(t)) {\n              b.delete(t);\n              i = \"cache-and-network\";\n            }\n\n            y.reexecuteOperation(toRequestPolicy(r, i));\n          }\n        }\n      });\n    }\n\n    function prepareForwardedOperation(e) {\n      if (\"query\" === e.kind) {\n        reserveLayer(h.data, e.key);\n      } else if (\"teardown\" === e.kind) {\n        O.delete(e.key);\n\n        _.delete(e.key);\n\n        noopDataState(h.data, e.key);\n      } else if (\"mutation\" === e.kind && \"network-only\" !== e.context.requestPolicy) {\n        var t = function writeOptimistic(e, t, r) {\n          if (\"production\" !== process.env.NODE_ENV) {\n            invariant(\"mutation\" === getMainOperation(t.query).operation, \"production\" !== process.env.NODE_ENV ? \"writeOptimistic(...) was called with an operation that is not a mutation.\\nThis case is unsupported and should never occur.\" : \"\", 10);\n          }\n\n          initDataState(\"write\", e.data, r, !0);\n          var i = startWrite(e, t, {}, void 0, !0);\n          clearDataState();\n          return i;\n        }(h, e, e.key).dependencies;\n\n        if (!function isDictEmpty(e) {\n          for (var t in e) {\n            return !1;\n          }\n\n          return !0;\n        }(t)) {\n          for (var n in t) {\n            E[n] = !0;\n          }\n\n          N.set(e.key, t);\n          var a = new Set();\n          collectPendingOperations(a, t);\n          executePendingOperations(e, a);\n        }\n      }\n\n      return i(e.kind, {\n        key: e.key,\n        query: r(e.query),\n        variables: e.variables ? filterVariables(getMainOperation(e.query), e.variables) : e.variables\n      }, e.context);\n    }\n\n    function updateDependencies(e, t) {\n      for (var r in t) {\n        (D[r] || (D[r] = [])).push(e.key);\n        O.set(e.key, e);\n      }\n    }\n\n    function operationResultFromCache(e) {\n      var t = query(h, e, _.get(e.key));\n      var r = t.data ? !t.partial ? \"hit\" : \"partial\" : \"miss\";\n\n      _.set(e.key, t.data);\n\n      updateDependencies(e, t.dependencies);\n      return {\n        outcome: r,\n        operation: e,\n        data: t.data,\n        dependencies: t.dependencies\n      };\n    }\n\n    function updateCacheWithResult(e, t) {\n      var r = e.operation;\n      var i = e.error;\n      var n = e.extensions;\n      var a = r.key;\n\n      if (\"mutation\" === r.kind) {\n        collectPendingOperations(t, N.get(a));\n        N.delete(a);\n      }\n\n      reserveLayer(h.data, r.key, e.hasNext);\n      var o;\n      var s = e.data;\n\n      if (s) {\n        collectPendingOperations(t, write(h, r, s, e.error, a).dependencies);\n        var u = query(h, r, \"query\" === r.kind ? _.get(r.key) || s : s, e.error, a);\n        s = u.data;\n\n        if (\"query\" === r.kind) {\n          collectPendingOperations(t, o = u.dependencies);\n\n          _.set(r.key, e.data);\n        }\n      } else {\n        noopDataState(h.data, r.key);\n      }\n\n      if (o) {\n        updateDependencies(e.operation, o);\n      }\n\n      return {\n        data: s,\n        error: i,\n        extensions: n,\n        operation: r\n      };\n    }\n\n    return function (e) {\n      var t = n(e);\n      var r = g ? n(a([o(s)(u(c(g))(t)), l(c(g))(t)])) : t;\n      var i = n(d(operationResultFromCache)(f(function (e) {\n        return \"query\" === e.kind && \"network-only\" !== e.context.requestPolicy;\n      })(r)));\n      var O = f(function (e) {\n        return \"query\" !== e.kind || \"network-only\" === e.context.requestPolicy;\n      })(r);\n\n      var _ = d(function (e) {\n        \"production\" !== process.env.NODE_ENV && m({\n          type: \"cacheMiss\",\n          message: \"The result could not be retrieved from the cache\",\n          operation: e.operation,\n          source: \"cacheExchange\"\n        });\n        return addCacheOutcome(e.operation, \"miss\");\n      })(f(function (e) {\n        return \"miss\" === e.outcome && \"cache-only\" !== e.operation.context.requestPolicy && !isBlockedByOptimisticUpdate(e.dependencies);\n      })(i));\n\n      var D = d(function (e) {\n        var t = e.operation;\n        var r = e.outcome;\n        var i = e.dependencies;\n        var n = {\n          operation: addCacheOutcome(t, r),\n          data: e.data,\n          error: e.error,\n          extensions: e.extensions\n        };\n\n        if (\"cache-and-network\" === t.context.requestPolicy || \"cache-first\" === t.context.requestPolicy && \"partial\" === r) {\n          n.stale = !0;\n\n          if (!isBlockedByOptimisticUpdate(i)) {\n            y.reexecuteOperation(toRequestPolicy(t, \"network-only\"));\n          } else if (\"cache-and-network\" === t.context.requestPolicy) {\n            b.add(t.key);\n          }\n        }\n\n        \"production\" !== process.env.NODE_ENV && m({\n          type: \"cacheHit\",\n          message: \"A requested operation was found and returned from the cache.\",\n          operation: e.operation,\n          data: {\n            value: n\n          },\n          source: \"cacheExchange\"\n        });\n        return n;\n      })(f(function (e) {\n        return \"miss\" !== e.outcome || \"cache-only\" === e.operation.context.requestPolicy;\n      })(i));\n      var w = n(v(d(prepareForwardedOperation)(a([O, _]))));\n      var F = d(function (e) {\n        var t = new Set();\n        var r = updateCacheWithResult(e, t);\n        executePendingOperations(e.operation, t);\n        return r;\n      })(f(function (e) {\n        return !N.has(e.operation.key);\n      })(w));\n      var S = o(function (e) {\n        if (k.push(e) < N.size) {\n          return p;\n        }\n\n        for (var t = 0; t < k.length; t++) {\n          reserveLayer(h.data, k[t].operation.key);\n        }\n\n        for (var r in E) {\n          delete E[r];\n        }\n\n        var i = [];\n        var n = new Set();\n        var a;\n\n        while (a = k.shift()) {\n          i.push(updateCacheWithResult(a, n));\n        }\n\n        executePendingOperations(e.operation, n);\n        return s(i);\n      })(f(function (e) {\n        return N.has(e.operation.key);\n      })(w));\n      return a([F, S, D]);\n    };\n  };\n}\n\nfunction isOfflineError(e) {\n  return e && e.networkError && !e.response && (\"undefined\" != typeof navigator && !1 === navigator.onLine || /request failed|failed to fetch|network\\s?error/i.test(e.networkError.message));\n}\n\nfunction offlineExchange(e) {\n  return function (r) {\n    var o = e.storage;\n\n    if (o && o.onOnline && o.readMetadata && o.writeMetadata) {\n      var s = r.forward;\n      var u = r.client;\n      var c = r.dispatchDebug;\n      var l = v();\n      var d = l.source;\n      var p = l.next;\n      var y = e.optimistic || {};\n      var m = [];\n\n      var updateMetadata = function () {\n        var e = [];\n\n        for (var t = 0; t < m.length; t++) {\n          var r = m[t];\n\n          if (\"mutation\" === r.kind) {\n            e.push({\n              query: g(r.query),\n              variables: r.variables\n            });\n          }\n        }\n\n        o.writeMetadata(e);\n      };\n\n      var h = !1;\n\n      var flushQueue = function () {\n        if (!h) {\n          h = !0;\n\n          for (var e = 0; e < m.length; e++) {\n            var t = m[e];\n\n            if (\"mutation\" === t.kind) {\n              p(i(\"teardown\", t));\n            }\n          }\n\n          for (var r = 0; r < m.length; r++) {\n            u.reexecuteOperation(m[r]);\n          }\n\n          m.length = 0;\n          h = !1;\n          updateMetadata();\n        }\n      };\n\n      o.onOnline(flushQueue);\n      o.readMetadata().then(function (e) {\n        if (e) {\n          for (var r = 0; r < e.length; r++) {\n            m.push(u.createRequestOperation(\"mutation\", t(e[r].query, e[r].variables)));\n          }\n\n          flushQueue();\n        }\n      });\n      var N = cacheExchange(e)({\n        client: u,\n        dispatchDebug: c,\n        forward: function (e) {\n          return f(function (e) {\n            if (\"mutation\" === e.operation.kind && isOfflineError(e.error) && function isOptimisticMutation(e, t) {\n              var r = t.variables || makeDict();\n              var i = getFragments(t.query);\n              var n = [].concat(getSelectionSet(getMainOperation(t.query)));\n              var a;\n\n              while (a = n.pop()) {\n                if (!shouldInclude(a, r)) {\n                  continue;\n                } else if (!isFieldNode(a)) {\n                  var o = !isInlineFragment(a) ? i[getName(a)] : a;\n\n                  if (o) {\n                    n.push.apply(n, getSelectionSet(o));\n                  }\n                } else if (e[getName(a)]) {\n                  return !0;\n                }\n              }\n\n              return !1;\n            }(y, e.operation)) {\n              m.push(e.operation);\n              updateMetadata();\n              return !1;\n            }\n\n            return !0;\n          })(s(e));\n        }\n      });\n      return function (e) {\n        var t = n(e);\n        var r = a([d, t]);\n        return f(function (e) {\n          if (\"query\" === e.operation.kind && isOfflineError(e.error)) {\n            p(toRequestPolicy(e.operation, \"cache-only\"));\n            m.push(e.operation);\n            return !1;\n          }\n\n          return !0;\n        })(N(r));\n      };\n    }\n\n    return cacheExchange(e)(r);\n  };\n}\n\nexport { Store, cacheExchange, offlineExchange, query, write };","map":{"version":3,"sources":["../src/ast/node.ts","../src/ast/variables.ts","../src/helpers/help.ts","../src/ast/traversal.ts","../src/ast/schema.ts","../src/ast/schemaPredicates.ts","../src/store/keys.ts","../src/helpers/dict.ts","../src/store/data.ts","../src/operations/shared.ts","../src/operations/write.ts","../src/operations/invalidate.ts","../src/store/store.ts","../src/operations/query.ts","../src/cacheExchange.ts","../src/helpers/operation.ts","../src/offlineExchange.ts"],"names":["node","getFieldAlias","getSelectionSet","selectionSet","value","isInlineFragment","const","kind","input","variableDefinitions","key","currentDebugStack","identifier","pop","length","env","code","doc","definitions","let","i","directives","directive","getName","vars","__schema","fields","buildType","name","type","types","possibleTypes","possibleType","mutationType","abstract","fieldName","expectObjectType","typename","schema","getField","typeCondition","field","invariant","process","NODE_ENV","warn","fieldName$1","resolverProperty","mutation","resolvers","validMutations","parentKey","parenIndex","entityKey","fieldKey","slice","newData","currentOwnership","makeNodeMap","currentDependencies","currentOptimistic","currentOptimisticKey","data","layerKey","initDataState","clearDataState","defer","gc","Set","storage","entity","undefined","updateRCForEntity","map","link","updateRCForLink","commutativeKeys","refCount","by","extractNodeMapFields","optimistic","batch","seenFieldKeys","refLock","currentData","writeLink","links","readRecord","setNode","reserveLayer","deferredKeys","optimisticOrder","splice","records","updateDependencies","currentOperation","writeData","hydrateData","writeRecord","persist","ctx","__typename","parentFieldKey","error","errorMap","__internal","updateContext","variables","childIterator","shouldInclude","childDeferred","select","deferRef","ref","result","store","normalizeVariables","writeOptimistic","writeSelection","fragments","fieldValue","resolver","writeField","getFieldError","updater","InMemoryData","opts","mutationName","query","invalidateEntity","subscription","l","expectValidUpdatesConfig","subscriptionName","id","createRequest","request","Store","fragment","writeFragment","args","invalidate","operation","rootKey","partial","rootNames","names","originalData","isQuery","iterate","readSelection","dataFieldValue","current","hasFields","_isListNullable","hasChanged","prevData","newLink","path","childLink","isDataOrKey","optimisticKeysToDependencies","operations","results","blockedDependencies","dependencies","pendingOperations","keys","prepareForwardedOperation","collectPendingOperations","dep","deps","hasNext","queryResult","queryDependencies","sharedOps$","ops$","inputOps$","cacheOps$","res","message","share","nonOptimisticResults$","filter","cacheResult","makeOperation","meta","toRequestPolicy","makeDict","fragmentNode","empty","bufferedResult","requests","isFlushingQueue","flushQueue","selections","forward","readMetadata","push","mutations","opsAndRebound$","failedQueue"],"mappings":";;;;;;;SAc6DA,O,CAAAA,C,EAAAA;;;;AAG9B,SAAA,mBAAA,CAAA,CAAA,EAAA;SAGlBC,CAAAA,CAAAA,aAAAA,CAAAA,IAAAA,CAAAA,K;;;SAIAC,a,CAAAA,C,EAAAA;SAEQF,CAAAA,CAAKG,KAALH,GAAKG,CAAAA,CAAAA,KAAAA,CAAmBC,KAAxBJ,GAAwBI,OAAAA,CAAAA,CAAAA,C;;;SAIvBJ,e,CAAAA,C,EAAAA;;;;AAGI,SAAA,gBAAA,CAAA,CAAA,EAAA;SAEbK,CAAAA,CAAAA,aAAAA,GAAAA,OAAAA,CAAAA,CAAAA,CAAAA,aAAAA,CAAAA,GAAAA,I;;;;;;;ACpBXC,SAAAA,gBAAAA,CAAAA,CAAAA,EAAAA;WACYC,I,KAAAA,CAAAA,CAAAA,e;;;AAIRD,SAAAA,iBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;;;;;;;;;;;;;;;;;;AAsBFA,SAAAA,eAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;aACaE,CAAAA,CAAKC,mB,EAAAA;;;;;;eAWpBH,C,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,mBAAAA,CAAAA,M,EAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,E,EAAAA;;;;;;;;AAKIA,SAAAA,kBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;;;;;;;wBAUoCI,CAAAA,CAAAA,mBAAAA,CAAAA,M,EAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,E,EAAAA;;;;;;;;;QC/B7BA,C,IAAAA,CAAAA,CAAAA,CAAAA,C;;;;SAKqBC,C;;;;AAIhC,IAAA,CAAA,GAAA,IAAA,GAAA,EAAA;AACEC,IAAAA,CAAAA,GAAAA,EAAAA;;AAIAN,SAAAA,YAAAA,GAAAA;WACgBO,G;;;AAGjB,SAAA,aAAA,CAAA,CAAA,EAAA,CAAA,EAAA;;;;;;;SAQDF,IAAAA,CAAAA,CAAAA,IAAAA,KAAkBG,CAAAA,CAAAA,mBAAlBH,EAAkBG;;;;MASdF,C,EAAAA;;;;;AAID,SAAA,cAAA,GAAA;;;;AAID,SAAA,SAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;;;;iCAKGG,G,CAAAA,Q,EAAAA;;;;;;;;;;AClEH,SAAA,IAAA,CAAA,CAAA,EAAIC,CAAJ,EAAIA;SACKC,G,CAAIC,C,GAAAA;;;;;;;OAeVC,IAAIC,CAAAA,GAAI,C,EAAGA,CAAAA,GAAIH,CAAAA,CAAIC,WAAJD,CAAgBH,M,EAAQM,CAAAA,E,EAAAA;QACpCpB,CAAAA,CAAIkB,WAAJlB,CAAIkB,CAAJlB,EAAIkB,IAAJlB,KAAIkB,CAAAA,CAAAA,oB,EAAAA;;;;;;;;AAOb,SAAA,YAAA,CAAA,CAAA,EAAA;;;OAQMC,IAAIC,CAAAA,GAAI,C,EAAGpB,CAAAA,GAAIiB,CAAAA,CAAAA,WAAAA,CAAgBH,M,EAAQO,CAAAA,E,EAAAA;QACpCC,CAAAA,GAAAA,CAAAA,CAAAA,WAAAA,CAAAA,CAAAA,C;;;;;;;;;;;;YAoBRtB,CAAAA,CAAAA,UAAAA,CAAAA,CAAAA,C;YAGauB,OAAAA,CAAQF,CAARE,C;;;4BAM6B,C,EAAA,K,EAATC,C;sCAG3BlB,C;;;;;;;;;;;;;;;;;;;;cC9BKc,C;;;;;;;SCdXe,e,CAAAA,C,EAAAA,C,EAAAA,C,EAAAA;;;;;AAS4B,SAAA,cAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;;;;;;;;;;;AAY5BC,SAAAA,sBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;eACOE,CAAAA,CAAAA,OAAAA,CAAAA,IAAAA,C,IAAAA,MAAPD,CAAAA,CAAAA,OAAAA,CAAAA,IAAAA,C,IAAAA,CAAAA,CAAAA,QAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,C;;;SAGIE,iB,CAAAA,C,EAAAA,C,EAAAA,C,EAAAA;;;;;AAWJH,MAAAA,CAAAA,GAAAA,gBAAAA,CAAAA,CAAAA,CAAAA;;kBAEcI,C,EAAAA;;;;;;GAiEb,SAAA,kBAAA,CAAA,CAAA,EAAA,CAAA,EAAA;;GAAA,C,CAAA,E,CAAA,C;;SAnDMC,CAAAA,CAAAA,SAAAA,CAAAA,CAAAA,EAAAA,CAAAA,C;;;AAGT,SAASL,QAAT,CAASA,CAAT,EAA0BE,CAA1B,EAA0BA,CAA1B,EAA0BA;AACxBI,MAAAA,MAAAA,CAAAA,CAAAA,OAAAA,CAAAA,IAAAA,CAAAA,IAAAA,MAAAA,CAAAA,CAAAA,OAAAA,CAAAA,IAAAA,CAAAA,EAAAA;;;;AAWAA,EAAAA,gBAAAA,CACSZ,CADTY,EACSZ,CADTY,CAAAA;;;MAgB6B,iBAAzBC,OAAAA,CAAQ5B,GAAR4B,CAAYC,Q,EAAAA;SACTtC,C,EAAAA;;;;;;;;AAWR,SAAA,gBAAA,CAAA,CAAA,EAAA,CAAA,EAAA;;;;AA6ES,SAAA,iBAAA,CAAiBsB,CAAjB,EAAiBA;;;;ACnMhB,SAAA,UAAA,CAAA,CAAA,EAAA,CAAA,EAAA;;;;SASNyB,Q,CAASF,C,EAAAA,C,EAAAA;;;;AAIZ7C,SAAAA,cAAAA,CAAAA,CAAAA,EAAAA;MACMgD,CAAAA,GAAAA,CAAAA,CAAAA,OAAAA,CAAAA,GAAAA,C;;;;;;;;;;;iBC7BNA,C;;;;;;AC2DFnC,SAAAA,kBAAAA,CAAAA,CAAAA,EAAAA;;;0BAEAA,C,EAAAA,O,CAAAA,M,EAAAA,G;gBACqBoC,K,CAAAA,CAAAA,GAAAA,C;;;;AAKnB,SAAA,QAAA,GAAA;;;;AASC,IAAA,CAAA,GAAA,IAAA;AACCC,IAAAA,CAAAA,GAAAA,IAAAA;AACD,IAAA,CAAA,GAAA,IAAA;;AAEDC,IAAAA,CAAAA,GAAAA,IAAAA;AACA,IAAA,CAAA,GAAA,IAAA;AACD,IAAA,CAAA,GAAA,CAAA,CAAA;;SAGCC,W,GAAAA;;;;;;;AAaAC,SAAAA,QAAAA,CAAAA,CAAAA,EAAAA;AACAC,MAAAA,CAAAA;;MACIjB,C,EAAAA;AACFhC,QAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,EAAAA;;;;AAIAkD,IAAAA,CAAAA,GAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,CAAAA,EAAAA,EAAAA,CAAAA,CAAAA;aAC6BL,C;;;;;IAKdM,G,CAADN,C;;;;;AAIZM,SAAAA,QAAAA,CAAAA,CAAAA,EAAAA;;;;;;UAMqBC,G;;;;;;MAMvB,iBAAA,OAAA,CAAA,GAAA,CAAA,Q,EAAA;;;;;;;;mBAaeD,C,EAAAA,C;;;;;;;iBAibbwB,CAAAA,CAAJK,eAAIL,CAAJK,OAAIL,CAAJK,CAAIL,C,EAAJK;AACEL,QAAAA,CAAAA,CAAKK,eAALL,CAAKK,OAALL,CAAKK,CAALL;;;;;;;;;;;gBA7ZYxB,C,EAAAA,C;;;;AAGZnD,SAAAA,cAAAA,GAAAA;;;;;;MAOEqD,CAAAA,GAAAA,C;;;;MAIAF,CAAAA,IAAAA,CAAAA,CAAAA,eAAAA,CAAAA,OAAAA,CAAAA,CAAAA,IAAAA,CAAAA,C,EAAAA;;;kBAWY,C,IAAA,CAAA,CAAA,OAAA,CAAA,CAAA,CAAA,eAAA,CAAA,CAAA,CAAA,C,IAAA,CAAA,CAAA,eAAA,CAAA,GAAA,CAAA,CAAA,CAAA,eAAA,CAAA,CAAA,CAAA,C,IAAA,CAAA,CAAA,CAAA,YAAA,CAAA,GAAA,CAAA,CAAA,CAAA,eAAA,CAAA,CAAA,CAAA,C,EAAA;;;;;AAEhBG,EAAAA,CAAAA,GAAAA,IAAAA;;;AAIAvB,EAAAA,CAAAA,GAAAA,IAAAA;;;;eAcqB,C;;;;AAWrBwB,IAAAA,CAAAA,CAAKA,KAALA,GAAKA,CAAAA,CAALA;AACAC,IAAAA,OAAAA,CAAQC,OAARD,GAAQC,IAARD,CAAQC,YAAAA;oBACC,M,EAAA,C,EAAA,I;OAwLP,SAAA,EAAA,GAAA;;;;;;;0BAaOe,O,EAAAA;AACLxB,gBAAAA,CAAAA,GAAAA,CAAAA,CAAAA,OAAAA,CAAAA,CAAAA,CAAAA;;;;;;;;;4BAgB0BL,C;mBACf8B,C;;;;;;;;;;;;OA/Bb,E;OAgPIc,SAAAA,WAAAA,GAAAA;;;;;YAKQC,O,CAAAA,O,CAAAA,UAAAA,CAAAA,EAAAA;AACV,gBAAA,CAAA,GAAA,kBAAA,CAAA,CAAA,CAAA;;;;;;;;;;0BCplBwBC,C;;;;;YAcPD,O,CAAAA,K;;ODgkBfD,E;;QApaCxC,K,GAAAA,CAAAA,C;KALPS;;;;AAUAE,SAAAA,aAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;;;;;;;;AAiBAlD,SAAAA,mBAAAA,GAAAA;;;;;;;;;;;;;;AAkDAnB,MAAAA,KAAOuE,CAAPvE,KAAIsE,CAAJtE,EAAIsE;iBAC6BhB,QAAAA,E;;;;;;;;;;AAiBM,SAAA,OAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;MACAa,C;;;;;AAYrCK,QAAAA,CAAAA,GAAiBC,CAAAA,CAAAA,UAAAA,CAAeC,CAAfD,CAAjBD;;;QAIIG,CAAAA,KAAAA,CAAgBR,CAAhBQ,IAAgBR,CAAAA,CAAAA,CAADS,eAACT,CAADS,GAACT,CAADS,CAACT,CAAhBQ,CAAAA,KAAeC,CAAAA,CAAAA,IAAAA,YAAAA,CAAAA,IAAAA,CAAAA,CAAAA,eAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAfD,KAAeC,KAAAA,CAAAA,MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAfD,IAAeC,CAAAA,IAAAA,C,EAAAA;aACV5E,CAAAA,CAAAA,CAAAA,C;;;;;;;AAcT,SAAKM,iBAAL,CAAKA,CAAL,EAA6BuE,CAA7B,EAA6BA,CAA7B,EAA6BA,CAA7B,EAA6BA;;iBAIMvB,CAAAA,GAADwB,CAACxB,GAADwB,C;;;;;;QAQhCC,M,CAAAA,C;;;;;AAWFzE,SAAAA,eAAAA,CAAuB0E,CAAvB1E,EAAuB0E,CAAvB1E,EAAuB0E,CAAvB1E,EAAuB0E,CAAvB1E,EAAuB0E;;;;;;;;;;;;;;AAcvB1E,SAAAA,iBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;aACSgD,C,IAAAA,C,EAAAA;AACP2B,UAAAA,CAAKC,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAALD,EAAKC;;;;;;;;AAUL,SAAA,oBAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;;;sBACgB7B,CAAAA,CAAAA,eAAAA,CAAAA,M,EAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,E,EAAAA;;;;;;;;;SAmETgC,kB,CAAAA,C,EAAAA,C,EAAAA;uBAKLvB,C,EAAAA;;AAEFe,MAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA;;AAEAS,MAAAA,CAAAA,CAAAA,QAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA;;;;;;;AAMFT,IAAAA,CAAAA,CAAAA,OAAAA,CAAAA,GAAAA,CAAAA,SAAAA,aAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;uBAraJ1D,K,EAAAA,K,IAAAA,G,GAAAA,C;AAsamB0C,KADfgB,CACehB,CADfgB,EACehB,CADfgB,CAAAA;;;;AAMAS,SAAKC,UAALD,CAAKC,CAALD,EAAKC,CAALD,EAAKC;;;;;AAKPjF,SAAAA,QAAAA,CAAkB+C,CAAlB/C,EAAkB+C,CAAlB/C,EAAkB+C;;;;;;AAOlBmC,EAAAA,kBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA;;AAEAb,EAAAA,OAAAA,CAAAA,CAAAA,CAAoBE,OAApBF,EAAoBE,CAApBF,EAAwCrB,CAAxCqB,EAAwCrB,CAAxCqB,CAAAA;;;AAYA,SAAA,SAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;;;;;;;;;;AAeEb,IAAAA,CAAAA,GAAAA,CAAAA,CAAAA,QAAAA;;AAEAA,IAAAA,CAAAA,GAAK4B,CAAAA,CAAAA,EAAL5B;;;;;;;;;;;;;;;;AAyBAA,IAAAA,CAAAA,CAAK4B,eAAL5B,CAAK4B,OAAL5B,CAAK4B,CAAL5B;;kBAKiBA,C;;AAGjBA,IAAAA,CAAAA,CAAK6B,eAAL7B,CAAqB8B,OAArB9B,CAAqB8B,CAArB9B;;;;;;;;;;;;AAuBA+B,SAAAA,UAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;QACOvF,O,CAAAA,C,GAAAA;;;;;;;;AAOV,SAAA,WAAA,CAAA,CAAA,EAAA,CAAA,EAAA;;;;;;;;AAQCwF,EAAAA,UAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA;;;AAGAf,SAAAA,WAAAA,CAAAA,CAAAA,EAAAA;;;;;;AAQEgB,IAAAA,CAAAA,CAAAA,OAAAA,CAAAA,UAAAA,CAAAA,EAAAA,CAAAA,EAAAA;eACazC,C,IAAAA,C,EAAAA;;;KADbyC;;;;;;;eAQWzC,C,IAAAA,C,EAAAA;;;;;;MAOU0C,C;;;;SAKZC,a,CAAAA,C,EAAAA;AAKXjC,MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA;;;;;;;;;;;WC/hBS;;AAEN,IAAA,CAAA,GAAA;;AAAA,CAAA;;AAIJ,SAAA,aAAA,CAAA,CAAA,EAAA;SAEY0C,CAAAA,CAAAA,UAAAA,CAAAA,IAAAA,CAAAA,MAAAA,GAAAA,CAAAA,IAAAA,CAAAA,CAAAA,UAAAA,CAAAA,QAAAA,GAAAA,CAAAA,CAAAA,UAAAA,CAAAA,QAAAA,CAAAA,CAAAA,CAAAA,UAAAA,CAAAA,IAAAA,CAAAA,IAAAA,CAAAA,GAAAA,CAAAA,CAAAA,GAAAA,KAAAA,C;;;AAUXN,SAAAA,WAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;AACAA,MAAIjD,CAAAA,GAAAA;YAAAA;eAEJwD,CAFIxD;gBAAAA;;;KAAAA;qBAAAA;gBAAAA;sBAAAA;iBAAAA;iBAAAA;eAAAA;mBAAAA;;;;;AAAAA,GAAJiD;;;;;;;;;;;;;;;;;;;AAsDE,SAAA,aAAA,CAAmBA,CAAnB,EAAmBA,CAAnB,EAAmBA,CAAnB,EAAmBA,CAAnB,EAAmBA,CAAnB,EAAmBA,CAAnB,EAAmBA;;;;;;;IAMjBQ,K,GAAAA,aAAAA,CAAAA,CAAAA,C;;;AAEA,SAAA,+BAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;;;;;;;;YAOKC,C;;;;;;;;;;YDiUV,SAAA,QAAA,CAAA,CAAA,EAAA,CAAA,EAAA;wBAGYpB,UAAAA,CAAAA,CAAAA,EAAAA,CAAAA,C,IAAAA,KAAAA,CAAAA,KAAAA,QAAAA,CAAAA,CAAAA,EAAAA,CAAAA,C;KAHZ,C,CAAA,E,CAAA,C;;;;AC3SWqB,SAAAA,qBAAAA,CAAAA,CAAAA,EAA6B9G,CAA7B8G,EAAsCC,CAAtCD,EAAsCC,CAAtCD,EAAsCC;;;UAEpCC,C;;;;;;;;;;;;;;;;AAqBRC,UAAAA,iBAAAA,OAAAA,CAAAA,GAAAA,CAAAA,QAAAA,EAAAA;;;;;;AAKAvC,UAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA;;;;;;;;gBAKuBuC,K,CAAAA,M,GAAAA,iBAAAA,CAAAA,CAAAA,CAAAA,KAAAA,CAAAA,MAAAA,EAAAA,CAAAA,EAAAA,CAAAA,C,GAAAA,+BAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,CAAAA,SAAAA,C,EAAAA;;;;;;;;;;;;;;;;;;;;;ACzJ3BhD,SAAAA,UAAAA,CAAAA,CAAAA,EAAAA;iBACOiD,C,GAAAA,I,GAAAA,C;;;AAUP5G,SAAAA,UAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;cACM4G,C,EAAAA;;;;;SAGG,IAAA,CAAA,GAAA,CAAA,EAAA,CAAA,GACPC,CAAAA,CACAC,M,EAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,E,EAAAA;;;;;;;;;;;;;;;;;;AA+BA,SAAA,KAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;;;;;;;;MAwBIhB,CAAAA,GAAAA,gBAAAA,CAAAA,CAAAA,CAAAA,KAAAA,C;AASNkB,MAAAA,CAAAA,GAAAA;WAAAA;kBAEgB1E,sBAAAA;AAFhB0E,GAAAA;;;;;;;;;;;;;;;;;AAsFI,SAAA,cAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;;;;;;iCAGU1E,Q,IAAAA,IAAAA,CACR6E,mJADQ7E,EACR6E,EADQ7E,C;;;mBAKW,Y,EAAA,C;;;gCAGaU,C,EAAAA,CAAAA,IAAAA,C,EAAAA,C,EAAAA,C;;;;QAShCoE,CAAAA,GAAAA,OAAAA,CAAAA,CAAAA,C;;;;;;;;qCAgBctB,Q,IAA4BjE,IAAAA,CAAAA,sCAAAA,CAAAA,GAAAA,oDAAAA,IAAAA,KAAAA,CAAAA,KAAAA,CAAAA,CAAAA,YAAAA,GAAAA,+BAAAA,GAAAA,eAAAA,IAAAA,kBAAAA,IAAAA,CAAAA,CAAAA,UAAAA,GAAAA,kDAAAA,GAAAA,EAAAA,CAAAA,EAAAA,EAAAA,C;;iBAC5CiE,CAAAA,CAAAA,KAAAA,CAAAA,MAAAA,IAAAA,CAAAA,IAAAA,iBAAAA,C,EAAAA;;;;;;;;;;;;gBA0BJW,K,CAAAA,mB,CAAAA,C;;;;;;;;;;;AAgBIvD,UAAAA,CAAAA,IAAAA,CAAAA,CAAAA,EAAAA;;;OAAAA,M;;;WAOKmE,IAAAA,CAAAA,IAAAA,CAAAA,CAAAA,EAAAA;;;;;AAaP9E,UAAI+E,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,OAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAJ/E;;;;;;;;;AChVKS,IAAAA,CAAAA,CAAAA,UAAAA,CAAAA,IAAAA,CAAoBzC,GAApByC;;;;AAIG,IAAA,CAAA,GAAA,iCAAA;;AACR,SAAA,UAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;MACEuE,KAAAA,CAAAA,OAAAA,CAAAA,CAAAA,C,EAAkCvE;;;;;;;;ACkChCE,MAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA;;;;;;;yBAMJrC,C,IAAAA,KAAAA,C,GAAAA,I;;;;MAGI2G,CAAAA,GAAJhE,CAAAA,CAAAA,U;;;AAGEiE,QAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAAAA,KAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAAAA,UAAAA,CAAAA,IAAAA,SAAAA,CAAAA,IAAAA,YAAAA,OAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAAAA,EAAAA;;;;;;;;;;AAwCF,SAAA,KAAA,CAAA,CAAA,EAAA;;;;;;;;;;OAMKjE,mB,GAAAA,CAAAA,CAAAA,UAAAA,IAAAA,E;;;;MACDuE,CAAAA,GAAmB,c;;;;cRnDV9H,CAAAA,CAAbkB,Q;;;;gBAIU,E;;wBAEML,CAAAA,GAAAA,CAAAA,CAAAA,M,EAAAA,CAAAA,E,EAAAA;YACVM,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,I,IAAAA,CAAAA,CAAAA,CAAAA,C;;;;;;eACEC,S,CAAAA,C,EAAAA;;;eAOD,W;mBACI;AACLC,cAAAA,IAAAA,EAAMC,CAAAA,CAAKD,IADN;AAELrB,cAAAA,IAAAA,EAAMsB,CAAAA,CAAKtB,IAFN;AAGLuB,cAAAA,UAAAA,EAAAA,YAAAA,CAAyBC,CAAAA,CAAAA,UAAAA,IAAN,EAAnBD,CAHK;;;;;;;;AAAA,a;;;;AAiBHE,cAAAA,IAAAA,EAAAA,CAAAA,CAAAA,I;;AACFF,cAAAA,KAAAA,EAAAA,YAAAA,CAAAA,CAAAA,CAAAA,aAAAA,IAAAA,EAAAA;;;;;cAQK;oDAAA;oBAEQG,Y,GAAAA,CAAAA,CAAAA,YAAAA,CAAAA,I,GAAAA,IAFR;yEAAA;qBAAA;;;AAOTR,cAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA;;;;iCAGaA,CAAAA,CAAAA,I,EAAAA;;;;;mBAGXS,CAAAA,KAAAA,C;;;AAbO,O;;;;;;;;;;;;;;;;;;;;;;;;;;oBQeKoG,C;;;;;oBAKKjC;YAAAA;eAAAA;;AAAAA,G;;;cJqIrB,SAAA,IAAA,CAAA,CAAA,EAAA;WACK;AACL/B,MAAAA,KAAAA,EAAAA,CAAOhB,CADF;mBAAA;wBAAA;qBAAA;0BAAA;yBAAA;0BAAA;4BAAA;6BAAA;gCAAA;yBAAA;;AAAA,K;GADL,C,CAAA,C;;WI/HuBhB,M,IAAAA,iBAAAA,OAAAA,CAAAA,GAAAA,CAAAA,Q,EAAAA;KPmBvBhC,SAAAA,uBAAAA,CAA+BgC,CAA/BhC,EAA+BgC,CAA/BhC,EAA+BgC;;aAC1BhC,IAAM6B,C,IAAAA,C,EAAAA;;yBACSzB,C,GAAAA;AAChBmC,cAAAA,IAAAA,CACE,oCAAA,CAAA,GAEE,oFAHJA,EAAI,EAAJA,CAAAA;;;;;KAHJvC,C,WAAAA,E,SAAAA,C;KAcAA,SAAAA,wBAAAA,CAAAA,CAAAA,EACEgC,CADFhC,EACEgC;;;;;;;;;;;;;;;;;;;;;;iBAuCOQ,C,IAAAA,C,EAAAA;;;;;;;;KAxCTxC,C,WAAAA,E,YAAAA,C;KA0DE,SAAA,0BAAA,CAAA,CAAA,EAAA,CAAA,EAAA;;;;;;;gBAYayC,K,EAAAA;sBACejB,K,CAAAA,CAAAA,CAAAA,K,EAAoBJ,M;;;;;;;;;;;;eAiBzCsB,IAAAA,gBAAAA,CAAAA,CAAAA,KAAAA,CAAAA,CAAAA,EAAAA,IAAAA,IAAAA,YAAAA,CAAAA,CAAAA,KAAAA,CAAAA,CAAAA,EAAAA,IAAAA,EAAAA;;SAAAA,M;;;eAIFE,IAAAA,C,IAALD,CAAAA,CAAAA,CAAAA,C,EAAAA;AACEJ,gBAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA;;;;;;;AAvCD,UAAA,CAAA,EAAA,CAAA;KAID,C,WAAA,E,KO1FiBI,SP0FjB,C;KCzN0D,SAAA,oCAAA,CAAA,CAAA,EAAA,CAAA,EAAA;uCAEvCE,Q,EAAAA;;;;UAIjBC,CAAAA,CAAAA,Q,EAAAA;;;;;;mBAKO,yCAAyCJ,CAAzC,GAAyCA,iG,EAAAA,E;;;;;KAXQ,C,WAAA,E,wBAAA,C;;;;;;;;;AMqJ5DiF,MAAAA,QAAAA,CAAAA,IAAAA,YAAAA,OAAAA,CAAAA,EAAAA;;;;;;;;;aAYgBM,U;;;;;kBAIGC,U,GAAAA;;;;;;;;;;;AAenBC,KAAAA,CAAAA,SAAAA,CAAAA,OAAAA,GAAmB,SAAA,OAAA,CAEFC,CAFE,EAEHjG,CAFG,EAEHA,CAFG,EAEHA;;;;AAWhBkG,MAAAA,CAAAA,CAAAA,EAAAA;;;;MAkBMC,CAAAA,GAAAA,UAAAA,CAAAA,CAAAA,EAAAA,CAAAA,C;;iBAIAvF,C,EAAAA;;;;;CAnCNoF;;ACpHFzE,KAAAA,CAAAA,SAAAA,CAAc6E,UAAd7E,GAA2B,SAAA,UAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;MACrBkD,CAAAA,GAAAA,KAAAA,WAAAA,CAAAA,CAAAA,C;AACNjD,EAAAA,SAAAA,CAAAA,CAAAA,EAAAA,iBAAAA,OAAAA,CAAAA,GAAAA,CAAAA,QAAAA,GAAAA,yHAAAA,OAAAA,CAAAA,IAAAA,QAAAA,GAAAA,CAAAA,CAAAA,UAAAA,GAAAA,CAAAA,GAAAA,IAAAA,GAAAA,EAAAA,EAAAA,EAAAA,CAAAA;GDQI+D,SAAKC,gBAALD,CAAKC,CAALD,EAAKC,CAALD,EAAKC,CAALD,EAAKC;;AAELC,MAAAA,QAAAA,EAAAA,UAAAA,CAAAA,CAAAA,EAAAA,CAAAA;;;SASGpE,IAAL1C,CAAAA,GAAAA,CAAK0C,EAAOqE,CAAAA,GAAAA,CAAAA,CAAZ/G,M,EAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,E,EAAAA;;;AAIEgH,UAAAA,KAAsC7D,CAAtC6D,KAAsC7D,QAAAA,CAAAA,CAAAA,EAAdjB,CAAciB,CAAtC6D,EAAwB9E;;OAAxB8E,M;;;;GAfAJ,C,CAAAA,E,CAAAA,E,CAAAA,C;CCVJhE;;AAYA1D,KAAAA,CAAMwI,SAANxI,CAAMwI,aAANxI,GAAMwI,SAAAA,eAAAA,CAAAA,CAAAA,EAAAA;MACAC,CAAAA,GAAAA,KAAAA,WAAAA,CAAAA,CAAAA,C;2BAC6BD,C,IAAAA,E;CAFnCxI;;AAcAmI,KAAAA,CAAI9F,SAAJ8F,CAAI9F,WAAJ8F,GAAI9F,SAAAA,WAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;cACWqF,K,EAAAA,CAAAA,CAAAA,S;;;;;;;CADfS;;;;IASUT,K,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,KAAAA,C;;;;AAOTS,KAAAA,CAAAA,SAAAA,CAAAA,YAAAA,GAAAA,SAAAA,cAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;QA8KKpG,CAAAA,GAAY+G,YAAAA,CAAAA,CAAAA,C;;QAKdV,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,C;;;+CAGF7F,IAAAA,CACE,gIADFA,EACE,CADFA,C;;;;QAWIwG,CAAAA,GAAAA,mBAAAA,CAA+BX,CAA/BW,C;;;;;;;;;;;;;;oBAcYhH,C,EAAAA,C;;;YAILiH,aAAAA,CAAAA,WAAAA,CAAAA,CAAAA,EAAAA,CAAAA,IAAAA,EAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAPpJ,eAAAA,CAAAA,CAAAA,CAAOoJ,EAAPpJ,QAAAA,EAAOoJ,CAAAA,IAAPpJ,I;;;;;;;;CAnNLuI;;AAICO,KAAAA,CAAAA,SAAAA,CAAYL,aAAZK,GAAYL,SAAAA,eAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;GHqFdxH,SAAAA,aAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;YAEiBoG,CAAAA,CAAAA,MAAAA,CAAAA,IAAAA,CAAAA,CAAAA,EAAfjH,CAAeiH,CAAAA,C;;;;;;;;;;;;;;;;qCAoCC3E,Q,EAAAA;AAEb4E,MAAAA,aAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA;;;;;;;;GAxCLrG,C,IAAAA,E,IAAAA,E,CAAAA,E,CAAAA,C;CGrFE6H;;;;MAWI3G,IAAAA,GAAAA,KAAqB4G,CAArB5G,KAAqB4G,CAArB5G,GAAqB4G,CAArB5G,GAAqB4G,C;;;;;;;;AAU3B9H,SAAAA,KAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;wBACwBX,CAAAA,CAAxBF,I,EAAAA,C;;;;;;AAKE8F,SAAAA,IAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;;;yBAGAjF,kBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,SAAAA,C,EAAAA,YAAAA,CAAAA,CAAAA,CAAAA,KAAAA,C,EAAAA,C,EAAAA,C,EAAAA,CAAAA,C,EAAAA,C;;;;;;;;;;;;mCAgBAyB,Q,EAAAA;;;;;;;;;;;AAcA,SAAKzB,QAAL,CAAKA,CAAL,EAAKA,CAAL,EAAKA,CAAL,EAAKA,CAAL,EAAKA;;;;;;MAOHiF,C;;;;SAIGpG,CAAAA,GAAAA,CAAAA,E,EAAAA;;;;;;;;;;;;;;aAqBwBgI,CAAAA,KAAAA,C;;aAEJkB,C,KAAAA,C,EAAAA;;;;;;;;;;AAaI,SAAA,aAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;;YACb/B,IAAAA,KAAAA,CAAAA,CAAAA,CAAAA,MAAAA,C;;;sCAGdgB,C,EAAAA,CAAAA,E,EAAAA;;;;;;AAcE/B,MAAAA,CAAAA,CAAAA,UAAAA,CAAAA,IAAAA,CAAAA,GAAAA;;;;;;;;;;;;;oBA8BUA,C,EAAK+C,CAAAA,CAAAA,U,EAAAA,C,EAAAA,C;;;;AA4DnBhI,SAAAA,aAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;;;;MAK2BoD,iBAAAA,OAAAA,CAAAA,GAAAA,CAAAA,Q,EAAAA;;AAEzBgF,MAAAA,IAAAA,CAAAA,4DAAAA,CAAAA,GAAAA,0CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,UAAAA,CAAAA,QAAAA,GAAAA,SAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,UAAAA,CAAAA,YAAAA,GAAAA,kFAAAA,EAAAA,EAAAA,CAAAA;;;;MAQA7C,CAAAA,GAAAA,CAAAA,CAAAA,GAAAA,UAAAA,CAAAA,CAAAA,EAAAA,YAAAA,CAAAA,IAAAA,CAAAA,IAAAA,CAAAA,CAAAA,UAAAA,GAAAA,C;;;;;;;;;;;;;;;;;;;YAyCevG,aAAAA,CAAcH,CAAdG,C;;;;;YAKDgH,CAAAA,CAAAA,SAAAA,CAAAA,CAAAA,C;;;+BAcD7E,M,EAAAA,C,EAAAA,C;;;sBAcKkF,I,CAAAA,C;;;;;;4CASyBgC,Y,EAAAA;AAE3CC,MAAAA,CAAAA,GAAAA,CAAAA;WACK,IAAA,WACLF,mBAAAA,EADK,IACLA,CADK,IACLA,cAAAA,OAAAA,CAAAA,CAAAA,CAAAA,CADK,EACLA;AAMAA,MAAAA,aAAAA,CAAcnD,CAAdmD,EAAcnD,CAAdmD,EAAcnD,CAAdmD,EAAcnD,CAAdmD,EAAcnD,CAAdmD,EAAcnD,CAAdmD,CAAAA;;AAGAnD,UAAAA,KAAAA,CAAAA,KAAIK,CAAJL,EAAIK;;;;;;;AAQFA,QAAAA,CAAAA,GAAAA,qBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,eAAAA,CAAAA,CAAAA,CAAAA,EAAAA,KAAAA,CAAAA,KAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,QAAAA,CAAAA,CAAAA,CAAAA,CAAAA;;;sBAEqC,SAAA,C,IAAA,CAAA,eAAA,CAAA,CAAA,CAAA,MAAA,EAAA,CAAA,EAAA,CAAA,C,EAAA;;;KApBlC,M;AAwBLuC,MAAAA,CAAAA,GAAAA,CAAAA;oBACGzE,C,KAAAA,C,EAAAA;;;uBAiBoBlB,C,EAAAA,C;;;AAInBqG,QAAAA,CAAAA,GAAAA,WAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,eAAAA,CAAAA,CAAAA,CAAAA,EAAAA,KAAAA,CAAAA,KAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,QAAAA,CAAAA,CAAAA,CAAAA,CAAAA;;AAIFC,QAAAA,CAAAA,GAAAA,CAAAA;;;;0BAMuCH,O,EAAAA;AAWzCpD,MAAAA,CAAAA,GAAAA,CAAAA,CAAAA;;;;;;;;;gCAYyBwD,C;;;;;;;;;;;;;cAgCXlF,C,IAAAA,CAAAA,C,GAAAA,KAAAA,C,GAAAA,CAAAA,GAAAA,CAAAA,GAAAA,C;;;AAEhBpE,SAAAA,qBAAAA,CAA6B8F,CAA7B9F,EAA6B8F,CAA7B9F,EAA6B8F,CAA7B9F,EAA6B8F,CAA7B9F,EACyBgC,CADzBhC,EACiC+B,CADjC/B,EAC2C6B,CAD3C7B,EAC2C6B,CAD3C7B,EAC2C6B;;QAErC0H,CAAAA,GAAAA,CAAAA,CAAAA,K;uBAG+B1B,cAAAA,CAAAA,CAAAA,CAAAA,MAAAA,EAAAA,CAAAA,EAAAA,CAAAA,C,GAAAA,CAAAA,C;sBAEnCjB,CAAAA,CAAAA,M;;;kCAIExC,CAAAA,GAAIyD,C,EAFuB/G,CAAAA,E,EAAAA;AAU7BgF,MAAAA,CAAAA,CAAIK,UAAJL,CAAe0D,IAAf1D,CAAoBvF,IAApBuF,CAAoBvF,CAApBuF;;UAEI2D,CAAAA,GAAcxF,qBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,QAAAA,CAAAA,CAAAA,EAAAA,KAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,QAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,KAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,C;;;;;;;;;0BAWOqF,CAAAA,CAAAA,CAAAA,C;;;;;;;SAOzBI,IAAAA,CAAAA,IAAAA,SAAAA,CAAAA,EAAAA;;GAAAA,M,ICrgBH,SAAA,WAAA,CAAA,CAAA,EAAA;;GAAA,C,CAAA,C;;;SCtDDyB;qBAEEC,OAAAA,CAAAA,GAAAA,CAAAA,Q,IAAAA,IAAAA,CAAAA,2CAAAA,CAAAA,GAAAA,qGAAAA,EAAAA,CAAAA,C;;;;;SAOSC,W,CAAAA,C,EAAAA,C,EAAAA,C,EAAAA,C,EAAAA,C,EAAAA,C,EAAAA,C,EAAAA;;;;;;;;wBDyBgB7D,I,CAA3BxH,C;;;;wBAKsBwD,G;;;AAIhBmG;;AAEAC,QAAAA,CAAAA,CAAAA,OAAAA,GAAAA,CAAAA,CAAAA,OAAAA,IAAAA,KAAAA,CAAAA,KAAAA,CAAAA,IAAAA,CAAAA;AACAC,QAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,IAAAA,IAAAA;AACAC,QAAAA,CAAAA,GAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAAAA,CAAAA,KAAAA,CAAAA,CAAAA,CAAAA,CAAAA;;;;;aAKcC,SAAAA,CAAAA,IAAAA,SAAAA,CAAAA,IAAAA,C,EAAAA;;;;;;;;;;;;;;;AAeVC,SAAAA,eAAAA,CAAAA,CAAAA,EAAsBC,CAAtBD,EAAsBC;;;;;;;;;;;YAclB,IAAA,KAAA,CAAA,CAAA,C;;;;;SLnBFlE,SAAAA,WAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;;;AAGVC,gBAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA;;AAEAC,gBAAAA,KAAAA,CAAAA,KAAAA,CAAAA,EAAAA;;;;;AAKEC,kBAAAA,QAAAA,CAAAA,CAAUjC,CAAViC,CAAAA,EAAUjC;;eAAViC,M;;;;;;;sBAQWC,C;SAlBHJ,C,MAAAA,E,SAAAA,E,CAAAA,C;;;;;;;;;;;;;;aKkCNmE,2B,CAAAA,C,EAAAA;;;;;;;;;;aAUGC,wB,CAAAA,C,EAAAA,C,EAAAA;;;;;;;;;;;;;;;;;;;;;;;;;;;AAyCCC,cAAAA,CAAAA,GAASC,mBAATD;;;;;;;;;;;;;;;;;;;yBJnBUrD,e,CAAAA,C,EAAAA,C,EAAAA,C,EAAAA;;;;;;;;;;;;;;;;;;;cI0DQqD,C,IAAAA,CAAAA,C;;;kBAIqBE,G,EAAAA,C;AAG3C9G,cAAIwG,CAAAA,GAAAA,IAAAA,GAAAA,EAAJxG;;AACAA,UAAAA,wBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA;;;;;eAOepD,G;AAQjBoD,QAAAA,KAAAA,EAAO+G,CAAAA,CAAAA,CAAAA,CAAAA,KAAAA,C;;;;;;oBAKO/B,C,EAAAA;;;;;;;UAOZgC,CAAAA,GAAAA,KAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,C;;;;;;aAIG;kBAAA;oBAAA;oBAAA;;AAAA,O;;;;UAIDC,CAAAA,GAAAA,CAAAA,CAAkBC,S;;;UAIlBC,CAAAA,GAAAA,CAAAA,CAAAA,G;;yBAcAC,CAAAA,CAAAA,I,EAAAA;;;;;;;UAgCArJ,CAAAA,GAAAA,CAAAA,CAAAA,I;;UAEAiH,C,EAAAA;AAba2B,QAAAA,wBAAAA,CAAAA,CAAAA,EAAAA,KAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,CAAAA,KAAAA,EAAAA,CAAAA,CAAAA,CAAAA,YAAAA,CAAAA;gBAIbzC,KAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,YAAAA,CAAAA,CAAAA,IAAAA,GAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,GAAAA,KAAAA,CAAAA,GAAAA,CAAAA,EAAAA,CAAAA,CAEgCzB,KAFhCyB,EAEgCzB,CAFhCyB,C;AAJayC,QAAAA,CAAAA,GAAAA,CAAAA,CAAAA,IAAAA;;;;;;;;;;;;;;;;;;;;;;;;UAyDXW,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,C;;;;;;2BAhCFD,CAAAA,CAAG5K,I,IAAAA,mBAAAA,CAAAA,CACH4K,OADG5K,CACH4K,a;SAJJD,C;;;AAiDW,yBAAA,OAAA,CAAA,GAAA,CAIXG,QAJW,IAIXA,CAAAA,CAAAA;2BAAAA;AAIIC,UAAAA,OAAAA,EAAAA,kDAJJD;gCAAAA;;AAAAA,SAAAA,CAJW;;;0BAiBFG,CAAAA,CAAAA,O,IAAAA,iBAAAA,CAAAA,CAAAA,SAAAA,CAAAA,OAAAA,CAAAA,a,IAAAA,CAAAA,2BAAAA,CAAAA,CAAAA,CAAAA,YAAAA,C;SAPTD,C;;;;;YAkBUzK,CAAAA,GAAAA,CAAAA,CAAAA,Y;;qBACOmJ,eAAAA,CAAAA,CAAAA,EAAAA,CAAAA,C;;;;;;;;;;;;AAYPE,YAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,GAAAA;;;;;AAKJA,UAAAA,IAAAA,EAAAA,U;;;;;;;;AAvB2B,eAAA,CAAA;;;;;gBElVrCrB,UAAAA,CAAAA,EAAAA;gBAEwB,IAAuB8C,GAAvB,E;;;;SAMjB/E,CAAAA,CAAAA,UAAAA,CAAAA,EAAAA;;AAAAA,OAAAA,CAAAA,CAAAA,CAAAA,C;UAMCgF,CAAAA,GAAAA,CAAAA,CAAAA,UAAAA,CAAAA,EAAAA;cAAiC3L,I,CAAAA,C,IACtB+J,CAAAA,CAAAA,I,EAAAA;iBACf6B,C;;;;;;;sBAOkBvF,C,EAAAA;;;;;;;;AAepBlC,eAAO0H,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,EAAP1H,EAAO0H;;;;;iBAMsB5B,C;OA9BvB0B,CAAAA,C;;WAAAA,C;;;;;;;;;;;;;;;;;UAqEAO,CAAAA,GAAAA,CAAAA,CAAAA,a;;gBAKgB7L,M;;;cAMd,E;;;;;;;;cASR,eAAQ8L,CAAAA,CAAAA,I,EAAAA;AACFE,YAAAA,CAAAA,CAAJD,IAAIC,CAAJD;uBACmBxD,CAAAA,CAAAA,K,CADnBwD;2BAGY3F;AAHZ2F,aAAIC;;;;;;;;;;;;;eA0BFhB,IAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,M,EAAAA,CAAAA,E,EAAAA;oBACoBhL,CAAAA,CAAAA,CAAAA,C;;AAEhBkM,gBAAAA,eAAAA,CAAAA,CAAAA,IAAAA,EAAAA;gBACOhB,CAAAA,CAAAA,UAAAA,EAAAA,CAAAA,C;;;;AANb,eAAA,IAAA,CAAA,GACgBe,CADhB,EACgBA,CAAAA,GAAAA,CAAAA,CAAAA,MADhB,EACgBA,CAAAA,EADhB,EACgBA;;;;qBAcOhM,C;;;;;;;;;;;;;;;;;;;;;8EA1FnBwL,SAAAA,oBAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;qCACwBJ,QAAAA,E;;;;;;sCAKLI,C,GAAAA;;;sBAGrBC,CAAAA,GAAAA,CAAAA,gBAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,OAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,C;;sBACEC,C,EAAAA;iCACkBC,C,EAAAA,eAAAA,CAAAA,CAAAA,C;;;;;;;;aAXlBH,C,CAAAA,E,WAAAA,C","sourcesContent":["import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  Kind,\n} from 'graphql';\n\nexport type SelectionSet = ReadonlyArray<SelectionNode>;\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias ? node.alias.value : getName(node);\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet => (node.selectionSet ? node.selectionSet.selections : []);\n\nexport const getTypeCondition = (node: {\n  typeCondition?: NamedTypeNode;\n}): string | null => (node.typeCondition ? getName(node.typeCondition) : null);\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  const args = {};\n  let argsSize = 0;\n  if (node.arguments && node.arguments.length) {\n    for (let i = 0, l = node.arguments.length; i < l; i++) {\n      const arg = node.arguments[i];\n      const value = valueFromASTUntyped(arg.value, vars);\n      if (value !== undefined && value !== null) {\n        args[getName(arg)] = value;\n        argsSize++;\n      }\n    }\n  }\n\n  return argsSize > 0 ? args : null;\n};\n\n/** Returns a filtered form of variables with values missing that the query doesn't require */\nexport const filterVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n) => {\n  if (!input || !node.variableDefinitions) {\n    return undefined;\n  }\n\n  const vars = {};\n  for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n    const name = getName(node.variableDefinitions[i].variable);\n    vars[name] = input[name];\n  }\n\n  return vars;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n): Variables => {\n  const vars = {};\n  if (!input) return vars;\n\n  if (node.variableDefinitions) {\n    for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n      const def = node.variableDefinitions[i];\n      const name = getName(def.variable);\n      vars[name] =\n        input[name] === undefined && def.defaultValue\n          ? valueFromASTUntyped(def.defaultValue, input)\n          : input[name];\n    }\n  }\n\n  for (const key in input) {\n    if (!(key in vars)) vars[key] = input[key];\n  }\n\n  return vars;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/graphcache/errors.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\n\nexport type ErrorCode =\n  | 1\n  | 2\n  | 3\n  | 4\n  | 5\n  | 6\n  | 7\n  | 8\n  | 9\n  | 10\n  | 11\n  | 12\n  | 13\n  | 14\n  | 15\n  | 16\n  | 17\n  | 18\n  | 19\n  | 20\n  | 21\n  | 22\n  | 23\n  | 24\n  | 25\n  | 26;\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\n// URL unfurls to https://formidable.com/open-source/urql/docs/graphcache/errors/\nconst helpUrl = '\\nhttps://bit.ly/2XbVrpR#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const popDebugNode = () => currentDebugStack.pop();\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import {\n  SelectionNode,\n  DocumentNode,\n  OperationDefinitionNode,\n  FragmentSpreadNode,\n  InlineFragmentNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { invariant } from '../helpers/help';\nimport { Fragments, Variables } from '../types';\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  for (let i = 0; i < doc.definitions.length; i++) {\n    if (doc.definitions[i].kind === Kind.OPERATION_DEFINITION) {\n      return doc.definitions[i] as OperationDefinitionNode;\n    }\n  }\n\n  invariant(\n    false,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments => {\n  const fragments: Fragments = {};\n  for (let i = 0; i < doc.definitions.length; i++) {\n    const node = doc.definitions[i];\n    if (node.kind === Kind.FRAGMENT_DEFINITION) {\n      fragments[getName(node)] = node;\n    }\n  }\n\n  return fragments;\n};\n\n/** Resolves @include and @skip directives to determine whether field is included. */\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0; node.directives && i < node.directives.length; i++) {\n    const directive = node.directives[i];\n    const name = getName(directive);\n    if (\n      (name === 'include' || name === 'skip') &&\n      directive.arguments &&\n      directive.arguments[0] &&\n      getName(directive.arguments[0]) === 'if'\n    ) {\n      // Return whether this directive forces us to skip\n      // `@include(if: false)` or `@skip(if: true)`\n      const value = valueFromASTUntyped(directive.arguments[0].value, vars);\n      return name === 'include' ? !!value : !value;\n    }\n  }\n\n  return true;\n};\n\n/** Resolves @defer directive to determine whether a fragment is potentially skipped. */\nexport const isDeferred = (\n  node: FragmentSpreadNode | InlineFragmentNode,\n  vars: Variables\n): boolean => {\n  for (let i = 0; node.directives && i < node.directives.length; i++) {\n    const directive = node.directives[i];\n    const name = getName(directive);\n    if (name === 'defer') {\n      for (\n        let j = 0;\n        directive.arguments && j < directive.arguments.length;\n        j++\n      ) {\n        const argument = directive.arguments[i];\n        if (getName(argument) === 'if') {\n          // Return whether `@defer(if: )` is enabled\n          return !!valueFromASTUntyped(argument.value, vars);\n        }\n      }\n\n      return true;\n    }\n  }\n\n  return false;\n};\n","import {\n  IntrospectionQuery,\n  IntrospectionSchema,\n  IntrospectionInputValue,\n  IntrospectionTypeRef,\n  IntrospectionType,\n} from 'graphql';\n\nexport interface SchemaField {\n  name: string;\n  type: IntrospectionTypeRef;\n  args: Record<string, IntrospectionInputValue>;\n}\n\nexport interface SchemaObject {\n  name: string;\n  kind: 'INTERFACE' | 'OBJECT';\n  interfaces: Record<string, unknown>;\n  fields: Record<string, SchemaField>;\n}\n\nexport interface SchemaUnion {\n  name: string;\n  kind: 'UNION';\n  types: Record<string, unknown>;\n}\n\nexport interface SchemaIntrospector {\n  query: string | null;\n  mutation: string | null;\n  subscription: string | null;\n  types?: Record<string, SchemaObject | SchemaUnion>;\n  isSubType(abstract: string, possible: string): boolean;\n}\n\nexport interface PartialIntrospectionSchema {\n  queryType: { name: string; kind?: any };\n  mutationType?: { name: string; kind?: any } | null;\n  subscriptionType?: { name: string; kind?: any } | null;\n  types?: IntrospectionSchema['types'];\n}\n\nexport type IntrospectionData =\n  | IntrospectionQuery\n  | { __schema: PartialIntrospectionSchema };\n\nexport const buildClientSchema = ({\n  __schema,\n}: IntrospectionData): SchemaIntrospector => {\n  const typemap: Record<string, SchemaObject | SchemaUnion> = {};\n\n  const buildNameMap = <T extends { name: string }>(\n    arr: ReadonlyArray<T>\n  ): { [name: string]: T } => {\n    const map: Record<string, T> = {};\n    for (let i = 0; i < arr.length; i++) map[arr[i].name] = arr[i];\n    return map;\n  };\n\n  const buildType = (\n    type: IntrospectionType\n  ): SchemaObject | SchemaUnion | void => {\n    switch (type.kind) {\n      case 'OBJECT':\n      case 'INTERFACE':\n        return {\n          name: type.name,\n          kind: type.kind as 'OBJECT' | 'INTERFACE',\n          interfaces: buildNameMap(type.interfaces || []),\n          fields: buildNameMap(\n            type.fields.map(field => ({\n              name: field.name,\n              type: field.type,\n              args: buildNameMap(field.args),\n            }))\n          ),\n        } as SchemaObject;\n      case 'UNION':\n        return {\n          name: type.name,\n          kind: type.kind as 'UNION',\n          types: buildNameMap(type.possibleTypes || []),\n        } as SchemaUnion;\n    }\n  };\n\n  const schema: SchemaIntrospector = {\n    query: __schema.queryType ? __schema.queryType.name : null,\n    mutation: __schema.mutationType ? __schema.mutationType.name : null,\n    subscription: __schema.subscriptionType\n      ? __schema.subscriptionType.name\n      : null,\n    types: undefined,\n    isSubType(abstract: string, possible: string) {\n      const abstractType = typemap[abstract];\n      const possibleType = typemap[possible];\n      if (!abstractType || !possibleType) {\n        return false;\n      } else if (abstractType.kind === 'UNION') {\n        return !!abstractType.types[possible];\n      } else if (\n        abstractType.kind !== 'OBJECT' &&\n        possibleType.kind === 'OBJECT'\n      ) {\n        return !!possibleType.interfaces[abstract];\n      } else {\n        return abstract === possible;\n      }\n    },\n  };\n\n  if (__schema.types) {\n    schema.types = typemap;\n    for (let i = 0; i < __schema.types.length; i++) {\n      const type = __schema.types[i];\n      if (type && type.name) {\n        const out = buildType(type);\n        if (out) typemap[type.name] = out;\n      }\n    }\n  }\n\n  return schema;\n};\n","import { InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, invariant } from '../helpers/help';\nimport { getTypeCondition } from './node';\nimport { SchemaIntrospector, SchemaObject } from './schema';\n\nimport {\n  KeyingConfig,\n  UpdateResolver,\n  ResolverConfig,\n  OptimisticMutationConfig,\n} from '../types';\n\nconst BUILTIN_NAME = '__';\n\nexport const isFieldNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  return !!field && field.type.kind !== 'NON_NULL';\n};\n\nexport const isListNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  if (!field) return false;\n  const ofType =\n    field.type.kind === 'NON_NULL' ? field.type.ofType : field.type;\n  return ofType.kind === 'LIST' && ofType.ofType.kind !== 'NON_NULL';\n};\n\nexport const isFieldAvailableOnType = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean =>\n  fieldName.indexOf(BUILTIN_NAME) === 0 ||\n  typename.indexOf(BUILTIN_NAME) === 0 ||\n  !!getField(schema, typename, fieldName);\n\nexport const isInterfaceOfType = (\n  schema: SchemaIntrospector,\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: string | void\n): boolean => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) {\n    return true;\n  } else if (\n    schema.types![typeCondition] &&\n    schema.types![typeCondition].kind === 'OBJECT'\n  ) {\n    return typeCondition === typename;\n  }\n\n  expectAbstractType(schema, typeCondition!);\n  expectObjectType(schema, typename!);\n  return schema.isSubType(typeCondition, typename);\n};\n\nconst getField = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n) => {\n  if (\n    fieldName.indexOf(BUILTIN_NAME) === 0 ||\n    typename.indexOf(BUILTIN_NAME) === 0\n  )\n    return;\n\n  expectObjectType(schema, typename);\n  const object = schema.types![typename] as SchemaObject;\n  const field = object.fields[fieldName];\n  if (!field) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n  }\n\n  return field;\n};\n\nfunction expectObjectType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types![typename] && schema.types![typename].kind === 'OBJECT',\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types![typename] &&\n      (schema.types![typename].kind === 'INTERFACE' ||\n        schema.types![typename].kind === 'UNION'),\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n\nexport function expectValidKeyingConfig(\n  schema: SchemaIntrospector,\n  keys: KeyingConfig\n): void {\n  if (process.env.NODE_ENV !== 'production') {\n    for (const key in keys) {\n      if (!schema.types![key]) {\n        warn(\n          'Invalid Object type: The type `' +\n            key +\n            '` is not an object in the defined schema, but the `keys` option is referencing it.',\n          20\n        );\n      }\n    }\n  }\n}\n\nexport function expectValidUpdatesConfig(\n  schema: SchemaIntrospector,\n  updates: Record<string, Record<string, UpdateResolver | undefined>>\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const mutationFields = (schema.types![schema.mutation] as SchemaObject)\n      .fields;\n    const givenMutations = updates[schema.mutation] || {};\n    for (const fieldName in givenMutations) {\n      if (mutationFields[fieldName] === undefined) {\n        warn(\n          'Invalid mutation field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Mutation` option is referencing it.',\n          21\n        );\n      }\n    }\n  }\n\n  if (schema.subscription) {\n    const subscriptionFields = (schema.types![\n      schema.subscription\n    ] as SchemaObject).fields;\n    const givenSubscription = updates[schema.subscription] || {};\n    for (const fieldName in givenSubscription) {\n      if (subscriptionFields[fieldName] === undefined) {\n        warn(\n          'Invalid subscription field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Subscription` option is referencing it.',\n          22\n        );\n      }\n    }\n  }\n}\n\nfunction warnAboutResolver(name: string): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` is not in the defined schema, but the \\`resolvers\\` option is referencing it.`,\n    23\n  );\n}\n\nfunction warnAboutAbstractResolver(\n  name: string,\n  kind: 'UNION' | 'INTERFACE'\n): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` does not match to a concrete type in the schema, but the \\`resolvers\\` option is referencing it. Implement the resolver for the types that ${\n      kind === 'UNION' ? 'make up the union' : 'implement the interface'\n    } instead.`,\n    26\n  );\n}\n\nexport function expectValidResolversConfig(\n  schema: SchemaIntrospector,\n  resolvers: ResolverConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  for (const key in resolvers) {\n    if (key === 'Query') {\n      if (schema.query) {\n        const validQueries = (schema.types![schema.query] as SchemaObject)\n          .fields;\n        for (const resolverQuery in resolvers.Query) {\n          if (!validQueries[resolverQuery]) {\n            warnAboutResolver('Query.' + resolverQuery);\n          }\n        }\n      } else {\n        warnAboutResolver('Query');\n      }\n    } else {\n      if (!schema.types![key]) {\n        warnAboutResolver(key);\n      } else if (\n        schema.types![key].kind === 'INTERFACE' ||\n        schema.types![key].kind === 'UNION'\n      ) {\n        warnAboutAbstractResolver(\n          key,\n          schema.types![key].kind as 'INTERFACE' | 'UNION'\n        );\n      } else {\n        const validTypeProperties = (schema.types![key] as SchemaObject).fields;\n        for (const resolverProperty in resolvers[key]) {\n          if (!validTypeProperties[resolverProperty]) {\n            warnAboutResolver(key + '.' + resolverProperty);\n          }\n        }\n      }\n    }\n  }\n}\n\nexport function expectValidOptimisticMutationsConfig(\n  schema: SchemaIntrospector,\n  optimisticMutations: OptimisticMutationConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const validMutations = (schema.types![schema.mutation] as SchemaObject)\n      .fields;\n    for (const mutation in optimisticMutations) {\n      if (!validMutations[mutation]) {\n        warn(\n          `Invalid optimistic mutation field: \\`${mutation}\\` is not a mutation field in the defined schema, but the \\`optimistic\\` option is referencing it.`,\n          24\n        );\n      }\n    }\n  }\n}\n","import { stringifyVariables } from '@urql/core';\nimport { FieldArgs, FieldInfo, KeyInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: FieldArgs) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const serializeKeys = (entityKey: string, fieldKey: string) =>\n  `${entityKey.replace(/\\./g, '%2e')}.${fieldKey}`;\n\nexport const deserializeKeyInfo = (key: string): KeyInfo => {\n  const dotIndex = key.indexOf('.');\n  const entityKey = key.slice(0, dotIndex).replace(/%2e/g, '.');\n  const fieldKey = key.slice(dotIndex + 1);\n  return { entityKey, fieldKey };\n};\n","export const makeDict = (): any => Object.create(null);\n\nexport const isDictEmpty = (x: any) => {\n  for (const _ in x) return false;\n  return true;\n};\n","import { stringifyVariables } from '@urql/core';\n\nimport {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n  Dependencies,\n  OperationType,\n  Data,\n} from '../types';\n\nimport {\n  serializeKeys,\n  deserializeKeyInfo,\n  fieldInfoOfKey,\n  joinKeys,\n} from './keys';\n\nimport { makeDict } from '../helpers/dict';\nimport { invariant, currentDebugStack } from '../helpers/help';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OptimisticMap<T> = Record<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OptimisticMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n}\n\nexport interface InMemoryData {\n  /** Flag for whether deferred tasks have been scheduled yet */\n  defer: boolean;\n  /** A list of entities that have been flagged for gargabe collection since no references to them are left */\n  gc: Set<string>;\n  /** A list of entity+field keys that will be persisted */\n  persist: Set<string>;\n  /** The API's \"Query\" typename which is needed to filter dependencies */\n  queryRootKey: string;\n  /** Number of references to each entity (except \"Query\") */\n  refCount: Dict<number>;\n  /** Number of references to each entity on optimistic layers */\n  refLock: OptimisticMap<Dict<number>>;\n  /** A map of entity fields (key-value entries per entity) */\n  records: NodeMap<EntityField>;\n  /** A map of entity links which are connections from one entity to another (key-value entries per entity) */\n  links: NodeMap<Link>;\n  /** A set of Query operation keys that are in-flight and deferred/streamed */\n  deferredKeys: Set<number>;\n  /** A set of Query operation keys that are in-flight and awaiting a result */\n  commutativeKeys: Set<number>;\n  /** The order of optimistic layers */\n  optimisticOrder: number[];\n  /** This may be a persistence adapter that will receive changes in a batch */\n  storage: StorageAdapter | null;\n}\n\nlet currentOwnership: null | Set<Data> = null;\nlet currentDataMapping: null | Map<Data, Data> = null;\nlet currentOperation: null | OperationType = null;\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Dependencies = null;\nlet currentOptimisticKey: null | number = null;\nlet currentOptimistic = false;\n\nconst makeNodeMap = <T>(): NodeMap<T> => ({\n  optimistic: makeDict(),\n  base: new Map(),\n});\n\n/** Creates a new data object unless it's been created in this data run */\nexport const makeData = (data?: Data): Data => {\n  let newData: Data;\n  if (data) {\n    if (currentOwnership!.has(data)) return data;\n    newData = currentDataMapping!.get(data) || ({ ...data } as Data);\n    currentDataMapping!.set(data, newData);\n  } else {\n    newData = {} as Data;\n  }\n\n  currentOwnership!.add(newData);\n  return newData;\n};\n\nexport const ownsData = (data?: Data): boolean =>\n  !!data && currentOwnership!.has(data);\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  operationType: OperationType,\n  data: InMemoryData,\n  layerKey?: number | null,\n  isOptimistic?: boolean\n) => {\n  currentOwnership = new Set();\n  currentDataMapping = new Map();\n  currentOperation = operationType;\n  currentData = data;\n  currentDependencies = makeDict();\n  currentOptimistic = !!isOptimistic;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  if (!layerKey) {\n    currentOptimisticKey = null;\n  } else if (isOptimistic || data.optimisticOrder.length > 0) {\n    // If this operation isn't optimistic and we see it for the first time,\n    // then it must've been optimistic in the past, so we can proactively\n    // clear the optimistic data before writing\n    if (!isOptimistic && !data.commutativeKeys.has(layerKey)) {\n      reserveLayer(data, layerKey);\n    } else if (isOptimistic) {\n      // NOTE: This optimally shouldn't happen as it implies that an optimistic\n      // write is being performed after a concrete write.\n      data.commutativeKeys.delete(layerKey);\n    }\n\n    // An optimistic update of a mutation may force an optimistic layer,\n    // or this Query update may be applied optimistically since it's part\n    // of a commutative chain\n    currentOptimisticKey = layerKey;\n    createLayer(data, layerKey);\n  } else {\n    // Otherwise we don't create an optimistic layer and clear the\n    // operation's one if it already exists\n    currentOptimisticKey = null;\n    deleteLayer(data, layerKey);\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  // NOTE: This is only called to check for the invariant to pass\n  if (process.env.NODE_ENV !== 'production') {\n    getCurrentDependencies();\n  }\n\n  const data = currentData!;\n  const layerKey = currentOptimisticKey;\n  currentOptimistic = false;\n  currentOptimisticKey = null;\n\n  // Determine whether the current operation has been a commutative layer\n  if (layerKey && data.optimisticOrder.indexOf(layerKey) > -1) {\n    // Squash all layers in reverse order (low priority upwards) that have\n    // been written already\n    let i = data.optimisticOrder.length;\n    while (\n      --i >= 0 &&\n      data.refLock[data.optimisticOrder[i]] &&\n      data.commutativeKeys.has(data.optimisticOrder[i]) &&\n      !data.deferredKeys.has(data.optimisticOrder[i])\n    ) {\n      squashLayer(data.optimisticOrder[i]);\n    }\n  }\n\n  currentOwnership = null;\n  currentDataMapping = null;\n  currentOperation = null;\n  currentData = null;\n  currentDependencies = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  // Schedule deferred tasks if we haven't already\n  if (process.env.NODE_ENV !== 'test' && !data.defer) {\n    data.defer = true;\n    Promise.resolve().then(() => {\n      initDataState('read', data, null);\n      gc();\n      persistData();\n      clearDataState();\n      data.defer = false;\n    });\n  }\n};\n\n/** Initialises then resets the data state, which may squash this layer if necessary */\nexport const noopDataState = (\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  if (layerKey && !isOptimistic) data.deferredKeys.delete(layerKey);\n  initDataState('read', data, layerKey, isOptimistic);\n  clearDataState();\n};\n\nexport const getCurrentOperation = (): OperationType => {\n  invariant(\n    currentOperation !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentOperation;\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Dependencies => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  defer: false,\n  gc: new Set(),\n  persist: new Set(),\n  queryRootKey,\n  refCount: makeDict(),\n  refLock: makeDict(),\n  links: makeNodeMap(),\n  records: makeNodeMap(),\n  deferredKeys: new Set(),\n  commutativeKeys: new Set(),\n  optimisticOrder: [],\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  const keymap: KeyMap<Dict<T | undefined>> = currentOptimisticKey\n    ? map.optimistic[currentOptimisticKey]\n    : map.base;\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  let node: Dict<T | undefined> | undefined;\n  // A read may be initialised to skip layers until its own, which is useful for\n  // reading back written data. It won't skip over optimistic layers however\n  let skip =\n    !currentOptimistic &&\n    currentOperation === 'read' &&\n    currentOptimisticKey &&\n    currentData!.commutativeKeys.has(currentOptimisticKey);\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const layerKey = currentData!.optimisticOrder[i];\n    const optimistic = map.optimistic[layerKey];\n    // If we're reading starting from a specific layer, we skip until a match\n    skip = skip && layerKey !== currentOptimisticKey;\n    // If the node and node value exists it is returned, including undefined\n    if (\n      optimistic &&\n      (!skip || !currentData!.commutativeKeys.has(layerKey)) &&\n      (!currentOptimistic ||\n        currentOperation === 'write' ||\n        currentData!.commutativeKeys.has(layerKey)) &&\n      (node = optimistic.get(entityKey)) !== undefined &&\n      fieldKey in node\n    ) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gc */\nconst updateRCForEntity = (\n  gc: void | Set<string>,\n  refCount: Dict<number>,\n  entityKey: string,\n  by: number\n): void => {\n  // Retrieve the reference count\n  const count = refCount[entityKey] !== undefined ? refCount[entityKey] : 0;\n  // Adjust it by the \"by\" value\n  const newCount = (refCount[entityKey] = (count + by) | 0);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gc !== undefined) {\n    if (newCount <= 0) gc.add(entityKey);\n    else if (count <= 0 && newCount > 0) gc.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gc */\nconst updateRCForLink = (\n  gc: void | Set<string>,\n  refCount: Dict<number>,\n  link: Link | undefined,\n  by: number\n): void => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gc, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      if (Array.isArray(link[i])) {\n        updateRCForLink(gc, refCount, link[i], by);\n      } else if (link[i]) {\n        updateRCForEntity(gc, refCount, link[i] as string, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n): void => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const optimistic = map.optimistic[currentData!.optimisticOrder[i]];\n    if (optimistic !== undefined) {\n      extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n    }\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = () => {\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  currentData!.gc.forEach((entityKey: string, _, batch: Set<string>) => {\n    // Check first whether the reference count is still 0\n    const rc = currentData!.refCount[entityKey] || 0;\n    if (rc > 0) {\n      batch.delete(entityKey);\n      return;\n    }\n\n    // Each optimistic layer may also still contain some references to marked entities\n    for (const layerKey in currentData!.refLock) {\n      const refCount = currentData!.refLock[layerKey];\n      const locks = refCount[entityKey] || 0;\n      // If the optimistic layer has any references to the entity, don't GC it,\n      // otherwise delete the reference count from the optimistic layer\n      if (locks > 0) return;\n      delete refCount[entityKey];\n    }\n\n    // Delete the reference count, and delete the entity from the GC batch\n    delete currentData!.refCount[entityKey];\n    batch.delete(entityKey);\n    currentData!.records.base.delete(entityKey);\n    const linkNode = currentData!.links.base.get(entityKey);\n    if (linkNode) {\n      currentData!.links.base.delete(entityKey);\n      for (const fieldKey in linkNode) {\n        updateRCForLink(batch, currentData!.refCount, linkNode[fieldKey], -1);\n      }\n    }\n  });\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies![entityKey] = true;\n    } else if (fieldKey !== undefined) {\n      currentDependencies![joinKeys(entityKey, fieldKey)] = true;\n    }\n  }\n};\n\nconst updatePersist = (entityKey: string, fieldKey: string) => {\n  if (!currentOptimistic && currentData!.storage) {\n    currentData!.persist.add(serializeKeys(entityKey, fieldKey));\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value?: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link?: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: Dict<number>;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gc: void | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount =\n      data.refLock[currentOptimisticKey] ||\n      (data.refLock[currentOptimisticKey] = makeDict());\n    links = data.links.optimistic[currentOptimisticKey];\n  } else {\n    refCount = data.refCount;\n    links = data.links.base;\n    gc = data.gc;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links && links.get(entityKey);\n  const prevLink = prevLinkNode && prevLinkNode[fieldKey];\n\n  // Update persistence batch and dependencies\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gc, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gc, refCount, link, 1);\n};\n\n/** Reserves an optimistic layer and preorders it */\nexport const reserveLayer = (\n  data: InMemoryData,\n  layerKey: number,\n  hasNext?: boolean\n) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index === -1) {\n    // The new layer needs to be reserved in front of all other commutative\n    // keys but after all non-commutative keys (which are added by `forceUpdate`)\n    data.optimisticOrder.unshift(layerKey);\n  } else if (!data.commutativeKeys.has(layerKey)) {\n    // Protect optimistic layers from being turned into non-optimistic layers\n    // while preserving optimistic data\n    clearLayer(data, layerKey);\n    // If the layer was an optimistic layer prior to this call, it'll be converted\n    // to a new non-optimistic layer and shifted ahead\n    data.optimisticOrder.splice(index, 1);\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  if (hasNext) {\n    data.deferredKeys.add(layerKey);\n  } else {\n    data.deferredKeys.delete(layerKey);\n  }\n\n  data.commutativeKeys.add(layerKey);\n};\n\n/** Creates an optimistic layer of links and records */\nconst createLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.optimisticOrder.indexOf(layerKey) === -1) {\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  if (!data.refLock[layerKey]) {\n    data.refLock[layerKey] = makeDict();\n    data.links.optimistic[layerKey] = new Map();\n    data.records.optimistic[layerKey] = new Map();\n  }\n};\n\n/** Clears all links and records of an optimistic layer */\nconst clearLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.refLock[layerKey]) {\n    delete data.refLock[layerKey];\n    delete data.records.optimistic[layerKey];\n    delete data.links.optimistic[layerKey];\n    data.deferredKeys.delete(layerKey);\n  }\n};\n\n/** Deletes links and records of an optimistic layer, and the layer itself */\nconst deleteLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index > -1) {\n    data.optimisticOrder.splice(index, 1);\n    data.commutativeKeys.delete(layerKey);\n  }\n\n  clearLayer(data, layerKey);\n};\n\n/** Merges an optimistic layer of links and records into the base data */\nconst squashLayer = (layerKey: number) => {\n  // Hide current dependencies from squashing operations\n  const previousDependencies = currentDependencies;\n  currentDependencies = makeDict();\n\n  const links = currentData!.links.optimistic[layerKey];\n  if (links) {\n    links.forEach((keyMap, entityKey) => {\n      for (const fieldKey in keyMap)\n        writeLink(entityKey, fieldKey, keyMap[fieldKey]);\n    });\n  }\n\n  const records = currentData!.records.optimistic[layerKey];\n  if (records) {\n    records.forEach((keyMap, entityKey) => {\n      for (const fieldKey in keyMap)\n        writeRecord(entityKey, fieldKey, keyMap[fieldKey]);\n    });\n  }\n\n  currentDependencies = previousDependencies;\n  deleteLayer(currentData!, layerKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const persistData = () => {\n  if (currentData!.storage) {\n    currentOptimistic = true;\n    currentOperation = 'read';\n    const entries: SerializedEntries = makeDict();\n    currentData!.persist.forEach(key => {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      let x: void | Link | EntityField;\n      if ((x = readLink(entityKey, fieldKey)) !== undefined) {\n        entries[key] = `:${stringifyVariables(x)}`;\n      } else if ((x = readRecord(entityKey, fieldKey)) !== undefined) {\n        entries[key] = stringifyVariables(x);\n      } else {\n        entries[key] = undefined;\n      }\n    });\n\n    currentOptimistic = false;\n    currentData!.storage.writeData(entries);\n    currentData!.persist.clear();\n  }\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState('write', data, null);\n\n  for (const key in entries) {\n    const value = entries[key];\n    if (value !== undefined) {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      if (value[0] === ':') {\n        writeLink(entityKey, fieldKey, JSON.parse(value.slice(1)));\n      } else {\n        writeRecord(entityKey, fieldKey, JSON.parse(value));\n      }\n    }\n  }\n\n  clearDataState();\n  data.storage = storage;\n};\n","import { CombinedError } from '@urql/core';\nimport {\n  GraphQLError,\n  FieldNode,\n  InlineFragmentNode,\n  FragmentDefinitionNode,\n} from 'graphql';\n\nimport {\n  isDeferred,\n  isInlineFragment,\n  getTypeCondition,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  isFieldNode,\n} from '../ast';\n\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\nimport { hasField } from '../store/data';\nimport { Store, keyOfField } from '../store';\n\nimport { getFieldArguments, shouldInclude, isInterfaceOfType } from '../ast';\n\nimport {\n  Fragments,\n  Variables,\n  DataField,\n  NullArray,\n  Link,\n  Entity,\n  Data,\n} from '../types';\n\nexport interface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  parent: Data;\n  fieldName: string;\n  error: GraphQLError | undefined;\n  partial: boolean;\n  optimistic: boolean;\n  __internal: {\n    path: Array<string | number>;\n    errorMap: { [path: string]: GraphQLError } | undefined;\n  };\n}\n\nexport const contextRef: { current: Context | null } = { current: null };\nexport const deferRef: { current: boolean } = { current: false };\n\n// Checks whether the current data field is a cache miss because of a GraphQLError\nexport const getFieldError = (ctx: Context): GraphQLError | undefined =>\n  ctx.__internal.path.length > 0 && ctx.__internal.errorMap\n    ? ctx.__internal.errorMap[ctx.__internal.path.join('.')]\n    : undefined;\n\nexport const makeContext = (\n  store: Store,\n  variables: Variables,\n  fragments: Fragments,\n  typename: string,\n  entityKey: string,\n  optimistic?: boolean,\n  error?: CombinedError | undefined\n): Context => {\n  const ctx: Context = {\n    store,\n    variables,\n    fragments,\n    parent: { __typename: typename },\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    error: undefined,\n    partial: false,\n    optimistic: !!optimistic,\n    __internal: {\n      path: [],\n      errorMap: undefined,\n    },\n  };\n\n  if (error && error.graphQLErrors) {\n    for (let i = 0; i < error.graphQLErrors.length; i++) {\n      const graphQLError = error.graphQLErrors[i];\n      if (graphQLError.path && graphQLError.path.length) {\n        if (!ctx.__internal.errorMap)\n          ctx.__internal.errorMap = Object.create(null);\n        ctx.__internal.errorMap![graphQLError.path.join('.')] = graphQLError;\n      }\n    }\n  }\n\n  return ctx;\n};\n\nexport const updateContext = (\n  ctx: Context,\n  data: Data,\n  typename: string,\n  entityKey: string,\n  fieldKey: string,\n  fieldName: string\n) => {\n  contextRef.current = ctx;\n  ctx.parent = data;\n  ctx.parentTypeName = typename;\n  ctx.parentKey = entityKey;\n  ctx.parentFieldKey = fieldKey;\n  ctx.fieldName = fieldName;\n  ctx.error = getFieldError(ctx);\n};\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  vars: Variables\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return !getSelectionSet(node).some(node => {\n    if (!isFieldNode(node)) return false;\n    const fieldKey = keyOfField(getName(node), getFieldArguments(node, vars));\n    return !hasField(entityKey, fieldKey);\n  });\n};\n\ninterface SelectionIterator {\n  (): FieldNode | undefined;\n}\n\nexport const makeSelectionIterator = (\n  typename: void | string,\n  entityKey: string,\n  select: SelectionSet,\n  ctx: Context\n): SelectionIterator => {\n  let childDeferred = false;\n  let childIterator: SelectionIterator | void;\n  let index = 0;\n\n  return function next() {\n    if (!deferRef.current && childDeferred) deferRef.current = childDeferred;\n\n    if (childIterator) {\n      const node = childIterator();\n      if (node != null) {\n        return node;\n      }\n\n      childIterator = undefined;\n      childDeferred = false;\n      if (process.env.NODE_ENV !== 'production') {\n        popDebugNode();\n      }\n    }\n\n    while (index < select.length) {\n      const node = select[index++];\n      if (!shouldInclude(node, ctx.variables)) {\n        continue;\n      } else if (!isFieldNode(node)) {\n        // A fragment is either referred to by FragmentSpread or inline\n        const fragmentNode = !isInlineFragment(node)\n          ? ctx.fragments[getName(node)]\n          : node;\n\n        if (fragmentNode !== undefined) {\n          const isMatching = ctx.store.schema\n            ? isInterfaceOfType(ctx.store.schema, fragmentNode, typename)\n            : isFragmentHeuristicallyMatching(\n                fragmentNode,\n                typename,\n                entityKey,\n                ctx.variables\n              );\n          if (isMatching) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(typename, fragmentNode);\n            }\n\n            childDeferred = !!isDeferred(node, ctx.variables);\n            if (!deferRef.current && childDeferred)\n              deferRef.current = childDeferred;\n\n            return (childIterator = makeSelectionIterator(\n              typename,\n              entityKey,\n              getSelectionSet(fragmentNode),\n              ctx\n            ))();\n          }\n        }\n      } else {\n        return node;\n      }\n    }\n  };\n};\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x == null ? null : (x as Data | NullArray<Data>);\n\nexport const ensureLink = (store: Store, ref: Link<Entity>): Link => {\n  if (ref == null) {\n    return ref;\n  } else if (Array.isArray(ref)) {\n    const link = new Array(ref.length);\n    for (let i = 0, l = link.length; i < l; i++)\n      link[i] = ensureLink(store, ref[i]);\n    return link;\n  }\n\n  const link = store.keyOfEntity(ref);\n  if (!link && ref && typeof ref === 'object') {\n    warn(\n      \"Can't generate a key for link(...) item.\" +\n        '\\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        ref.__typename +\n        '`.',\n      12\n    );\n  }\n\n  return link;\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n  isFieldAvailableOnType,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n} from '../ast';\n\nimport { invariant, warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  NullArray,\n  Variables,\n  Data,\n  Link,\n  OperationRequest,\n  Dependencies,\n  EntityField,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n  deferRef,\n} from './shared';\n\nexport interface WriteResult {\n  data: null | Data;\n  dependencies: Dependencies;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  key?: number\n): WriteResult => {\n  initDataState('write', store.data, key || null);\n  const result = startWrite(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  key: number\n): WriteResult => {\n  if (process.env.NODE_ENV !== 'production') {\n    invariant(\n      getMainOperation(request.query).operation === 'mutation',\n      'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n        'This case is unsupported and should never occur.',\n      10\n    );\n  }\n\n  initDataState('write', store.data, key, true);\n  const result = startWrite(store, request, {} as Data, undefined, true);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  isOptimistic?: boolean\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { data, dependencies: getCurrentDependencies() };\n  const kind = store.rootFields[operation.operation];\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    kind,\n    kind,\n    !!isOptimistic,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(kind, operation);\n  }\n\n  writeSelection(ctx, kind, getSelectionSet(operation), data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Partial<Data>,\n  variables?: Variables\n) => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (!fragment) {\n    return warn(\n      'writeFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      11\n    );\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const dataToWrite = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(dataToWrite);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey,\n    undefined\n  );\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), dataToWrite);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: undefined | string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.rootFields['query'];\n  const isRoot = !isQuery && !!ctx.store.rootNames[entityKey!];\n  const typename = isRoot || isQuery ? entityKey : data.__typename;\n  if (!typename) {\n    warn(\n      \"Couldn't find __typename when writing.\\n\" +\n        \"If you're writing to the cache manually have to pass a `__typename` property on each entity in your data.\",\n      14\n    );\n    return;\n  } else if (!isRoot && !isQuery && entityKey) {\n    InMemoryData.writeRecord(entityKey, '__typename', typename);\n  }\n\n  const iterate = makeSelectionIterator(\n    typename,\n    entityKey || typename,\n    select,\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iterate())) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldAlias = getFieldAlias(node);\n    let fieldValue = data[fieldAlias];\n\n    // Development check of undefined fields\n    if (process.env.NODE_ENV !== 'production') {\n      if (!isRoot && fieldValue === undefined && !deferRef.current) {\n        const advice = ctx.optimistic\n          ? '\\nYour optimistic result may be missing a field!'\n          : '';\n\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.' +\n            advice,\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.store.schema && typename && fieldName !== '__typename') {\n        isFieldAvailableOnType(ctx.store.schema, typename, fieldName);\n      }\n    }\n\n    if (\n      // Skip typename fields and assume they've already been written above\n      fieldName === '__typename' ||\n      // Fields marked as deferred that aren't defined must be skipped\n      (fieldValue === undefined && deferRef.current)\n    ) {\n      continue;\n    }\n\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n\n    // Execute optimistic mutation functions on root fields\n    if (ctx.optimistic && isRoot) {\n      const resolver = ctx.store.optimisticMutations[fieldName];\n\n      if (!resolver) continue;\n      // We have to update the context to reflect up-to-date ResolveInfo\n      updateContext(ctx, data, typename, typename, fieldKey, fieldName);\n      fieldValue = data[fieldAlias] = ensureData(\n        resolver(fieldArgs || {}, ctx.store, ctx)\n      );\n    }\n\n    if (node.selectionSet) {\n      // Process the field and write links for the child entities that have been written\n      if (entityKey && !isRoot) {\n        const key = joinKeys(entityKey, fieldKey);\n        const link = writeField(\n          ctx,\n          getSelectionSet(node),\n          ensureData(fieldValue),\n          key\n        );\n        InMemoryData.writeLink(entityKey || typename, fieldKey, link);\n      } else {\n        writeField(ctx, getSelectionSet(node), ensureData(fieldValue));\n      }\n    } else if (entityKey && !isRoot) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(\n        entityKey || typename,\n        fieldKey,\n        (fieldValue !== null || !getFieldError(ctx)\n          ? fieldValue\n          : undefined) as EntityField\n      );\n    }\n\n    if (isRoot) {\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        updateContext(\n          ctx,\n          data,\n          typename,\n          typename,\n          joinKeys(typename, fieldKey),\n          fieldName\n        );\n\n        data[fieldName] = fieldValue;\n        updater(data, fieldArgs || {}, ctx.store, ctx);\n      }\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n};\n\n// A pattern to match typenames of types that are likely never keyable\nconst KEYLESS_TYPE_RE = /^__|PageInfo|(Connection|Edge)$/;\n\nconst writeField = (\n  ctx: Context,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>,\n  parentFieldKey?: string\n): Link | undefined => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      // Add the current index to the walked path before processing the link\n      ctx.__internal.path.push(i);\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = parentFieldKey\n        ? joinKeys(parentFieldKey, `${i}`)\n        : undefined;\n      // Recursively write array data\n      const links = writeField(ctx, select, data[i], indexKey);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return newData;\n  } else if (data === null) {\n    return getFieldError(ctx) ? undefined : null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const typename = data.__typename;\n\n  if (\n    parentFieldKey &&\n    !ctx.store.keys[data.__typename] &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !KEYLESS_TYPE_RE.test(typename)\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  const childKey = entityKey || parentFieldKey;\n  writeSelection(ctx, childKey, select, data);\n  return childKey || null;\n};\n","import * as InMemoryData from '../store/data';\nimport { FieldArgs } from '../types';\nimport { keyOfField } from '../store';\n\ninterface PartialFieldInfo {\n  fieldKey: string;\n}\n\nexport const invalidateEntity = (\n  entityKey: string,\n  field?: string,\n  args?: FieldArgs\n) => {\n  const fields: PartialFieldInfo[] = field\n    ? [{ fieldKey: keyOfField(field, args) }]\n    : InMemoryData.inspectFields(entityKey);\n\n  for (let i = 0, l = fields.length; i < l; i++) {\n    const { fieldKey } = fields[i];\n    if (InMemoryData.readLink(entityKey, fieldKey) !== undefined) {\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n    } else {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    }\n  }\n};\n","import { DocumentNode } from 'graphql';\nimport { TypedDocumentNode, formatDocument, createRequest } from '@urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  FieldArgs,\n  Link,\n  Data,\n  QueryInput,\n  UpdateResolver,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  Entity,\n  CacheExchangeOpts,\n} from '../types';\n\nimport { invariant } from '../helpers/help';\nimport { contextRef, ensureLink } from '../operations/shared';\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidateEntity } from '../operations/invalidate';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\nimport {\n  SchemaIntrospector,\n  buildClientSchema,\n  expectValidKeyingConfig,\n  expectValidUpdatesConfig,\n  expectValidResolversConfig,\n  expectValidOptimisticMutationsConfig,\n} from '../ast';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport class Store<\n  C extends Partial<CacheExchangeOpts> = Partial<CacheExchangeOpts>\n> implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: Record<string, Record<string, UpdateResolver | undefined>>;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schema?: SchemaIntrospector;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(opts?: C) {\n    if (!opts) opts = {} as C;\n\n    this.resolvers = opts.resolvers || {};\n    this.optimisticMutations = opts.optimistic || {};\n    this.keys = opts.keys || {};\n\n    let queryName = 'Query';\n    let mutationName = 'Mutation';\n    let subscriptionName = 'Subscription';\n    if (opts.schema) {\n      const schema = buildClientSchema(opts.schema);\n      queryName = schema.query || queryName;\n      mutationName = schema.mutation || mutationName;\n      subscriptionName = schema.subscription || subscriptionName;\n      // Only add schema introspector if it has types info\n      if (schema.types) this.schema = schema;\n    }\n\n    this.updates = {\n      [mutationName]: (opts.updates && opts.updates.Mutation) || {},\n      [subscriptionName]: (opts.updates && opts.updates.Subscription) || {},\n    };\n\n    this.rootFields = {\n      query: queryName,\n      mutation: mutationName,\n      subscription: subscriptionName,\n    };\n\n    this.rootNames = {\n      [queryName]: 'query',\n      [mutationName]: 'mutation',\n      [subscriptionName]: 'subscription',\n    };\n\n    this.data = InMemoryData.make(queryName);\n\n    if (this.schema && process.env.NODE_ENV !== 'production') {\n      expectValidKeyingConfig(this.schema, this.keys);\n      expectValidUpdatesConfig(this.schema, this.updates);\n      expectValidResolversConfig(this.schema, this.resolvers);\n      expectValidOptimisticMutationsConfig(\n        this.schema,\n        this.optimisticMutations\n      );\n    }\n  }\n\n  keyOfField = keyOfField;\n\n  keyOfEntity(data: Entity) {\n    // In resolvers and updaters we may have a specific parent\n    // object available that can be used to skip to a specific parent\n    // key directly without looking at its incomplete properties\n    if (contextRef.current && data === contextRef.current.parent)\n      return contextRef.current!.parentKey;\n\n    if (data == null || typeof data === 'string') return data || null;\n    if (!data.__typename) return null;\n    if (this.rootNames[data.__typename]) return data.__typename;\n\n    let key: string | null | void;\n    if (this.keys[data.__typename]) {\n      key = this.keys[data.__typename](data);\n    } else if (data.id != null) {\n      key = `${data.id}`;\n    } else if (data._id != null) {\n      key = `${data._id}`;\n    }\n\n    return key ? `${data.__typename}:${key}` : null;\n  }\n\n  resolve(entity: Entity, field: string, args?: FieldArgs): DataField {\n    const fieldKey = keyOfField(field, args);\n    const entityKey = this.keyOfEntity(entity);\n    if (!entityKey) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link || null;\n  }\n\n  resolveFieldByKey = this.resolve;\n\n  invalidate(entity: Entity, field?: string, args?: FieldArgs) {\n    const entityKey = this.keyOfEntity(entity);\n\n    invariant(\n      entityKey,\n      \"Can't generate a key for invalidate(...).\\n\" +\n        'You have to pass an id or _id field or create a custom `keys` field for `' +\n        typeof entity ===\n        'object'\n        ? (entity as Data).__typename\n        : entity + '`.',\n      19\n    );\n\n    invalidateEntity(entityKey, field, args);\n  }\n\n  inspectFields(entity: Entity): FieldInfo[] {\n    const entityKey = this.keyOfEntity(entity);\n    return entityKey ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery<T = Data, V = Variables>(\n    input: QueryInput<T, V>,\n    updater: (data: T | null) => T | null\n  ): void {\n    const request = createRequest<T, V>(input.query, input.variables as any);\n    request.query = formatDocument(request.query);\n    const output = updater(this.readQuery(request));\n    if (output !== null) {\n      startWrite(this, request, output as any);\n    }\n  }\n\n  readQuery<T = Data, V = Variables>(input: QueryInput<T, V>): T | null {\n    const request = createRequest(input.query, input.variables!);\n    request.query = formatDocument(request.query);\n    return read(this, request).data as T | null;\n  }\n\n  readFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    entity: string | Data | T,\n    variables?: V\n  ): T | null {\n    return readFragment(\n      this,\n      formatDocument(fragment),\n      entity,\n      variables as any\n    ) as T | null;\n  }\n\n  writeFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    data: T,\n    variables?: V\n  ): void {\n    writeFragment(this, formatDocument(fragment), data, variables as any);\n  }\n\n  link(\n    entity: Entity,\n    field: string,\n    args: FieldArgs,\n    link: Link<Entity>\n  ): void;\n\n  link(entity: Entity, field: string, link: Link<Entity>): void;\n\n  link(\n    entity: Entity,\n    field: string,\n    argsOrLink: FieldArgs | Link<Entity>,\n    maybeLink?: Link<Entity>\n  ): void {\n    const args = (maybeLink !== undefined ? argsOrLink : null) as FieldArgs;\n    const link = (maybeLink !== undefined\n      ? maybeLink\n      : argsOrLink) as Link<Entity>;\n    const entityKey = ensureLink(this, entity);\n    if (typeof entityKey === 'string') {\n      InMemoryData.writeLink(\n        entityKey,\n        keyOfField(field, args),\n        ensureLink(this, link)\n      );\n    }\n  }\n}\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  Variables,\n  Data,\n  DataField,\n  Link,\n  OperationRequest,\n  Dependencies,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentOperation,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n  makeData,\n  ownsData,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n  deferRef,\n} from './shared';\n\nimport {\n  isFieldAvailableOnType,\n  isFieldNullable,\n  isListNullable,\n} from '../ast';\n\nexport interface QueryResult {\n  dependencies: Dependencies;\n  partial: boolean;\n  data: null | Data;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data | null | undefined,\n  error?: CombinedError | undefined,\n  key?: number\n): QueryResult => {\n  initDataState('read', store.data, key);\n  const result = read(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data | null | undefined,\n  error?: CombinedError | undefined\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.rootFields[operation.operation];\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    rootKey,\n    rootKey,\n    false,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  if (!input) input = makeData();\n  // NOTE: This may reuse \"previous result data\" as indicated by the\n  // `originalData` argument in readRoot(). This behaviour isn't used\n  // for readSelection() however, which always produces results from\n  // scratch\n  const data =\n    rootKey !== ctx.store.rootFields['query']\n      ? readRoot(ctx, rootKey, rootSelect, input)\n      : readSelection(ctx, rootKey, rootSelect, input);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: ctx.partial || !data,\n    data: data || null,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  input: Data\n): Data => {\n  const typename = ctx.store.rootNames[entityKey]\n    ? entityKey\n    : input.__typename;\n  if (typeof typename !== 'string') {\n    return input;\n  }\n\n  const iterate = makeSelectionIterator(entityKey, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasChanged = false;\n  const output = makeData(input);\n  while ((node = iterate())) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = input[fieldAlias];\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    if (node.selectionSet && fieldValue !== null) {\n      dataFieldValue = readRootField(\n        ctx,\n        getSelectionSet(node),\n        ensureData(fieldValue)\n      );\n    } else {\n      dataFieldValue = fieldValue;\n    }\n\n    // Check for any referential changes in the field's value\n    hasChanged = hasChanged || dataFieldValue !== fieldValue;\n    if (dataFieldValue !== undefined) output[fieldAlias] = dataFieldValue!;\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n\n  return hasChanged ? output : input;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: Link<Data>\n): Link<Data> => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    let hasChanged = false;\n    for (let i = 0, l = originalData.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the root field's value\n      newData[i] = readRootField(ctx, select, originalData[i]);\n      hasChanged = hasChanged || newData[i] !== originalData[i];\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return hasChanged ? newData : originalData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    return readSelection(ctx, entityKey, select, originalData) || null;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Partial<Data> | string,\n  variables?: Variables\n): Data | null => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (!fragment) {\n    warn(\n      'readFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      6\n    );\n\n    return null;\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename)\n    entity.__typename = typename;\n  const entityKey = store.keyOfEntity(entity as Data);\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey\n  );\n\n  const result =\n    readSelection(ctx, entityKey, getSelectionSet(fragment), makeData()) ||\n    null;\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nconst readSelection = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  input: Data,\n  result?: Data\n): Data | undefined => {\n  const { store } = ctx;\n  const isQuery = key === store.rootFields['query'];\n\n  const entityKey = (result && store.keyOfEntity(result)) || key;\n  if (!isQuery && !!ctx.store.rootNames[entityKey]) {\n    warn(\n      'Invalid root traversal: A selection was being read on `' +\n        entityKey +\n        '` which is an uncached root type.\\n' +\n        'The `' +\n        ctx.store.rootFields.mutation +\n        '` and `' +\n        ctx.store.rootFields.subscription +\n        '` types are special ' +\n        'Operation Root Types and cannot be read back from the cache.',\n      25\n    );\n  }\n\n  const typename = !isQuery\n    ? InMemoryData.readRecord(entityKey, '__typename') ||\n      (result && result.__typename)\n    : key;\n\n  if (typeof typename !== 'string') {\n    return;\n  } else if (result && typename !== result.__typename) {\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return;\n  }\n\n  const iterate = makeSelectionIterator(typename, entityKey, select, ctx);\n\n  let hasFields = false;\n  let hasPartials = false;\n  let hasChanged = typename !== input.__typename;\n  let node: FieldNode | void;\n  const output = makeData(input);\n  while ((node = iterate()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result ? result[fieldName] : undefined;\n    const resolvers = store.resolvers[typename];\n\n    if (process.env.NODE_ENV !== 'production' && store.schema && typename) {\n      isFieldAvailableOnType(store.schema, typename, fieldName);\n    }\n\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n\n    if (fieldName === '__typename') {\n      // We directly assign the typename as it's already available\n      dataFieldValue = typename;\n    } else if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (\n      getCurrentOperation() === 'read' &&\n      resolvers &&\n      typeof resolvers[fieldName] === 'function'\n    ) {\n      // We have to update the information in context to reflect the info\n      // that the resolver will receive\n      updateContext(ctx, output, typename, entityKey, key, fieldName);\n\n      // We have a resolver for this field.\n      // Prepare the actual fieldValue, so that the resolver can use it\n      if (fieldValue !== undefined) {\n        output[fieldAlias] = fieldValue;\n      }\n\n      dataFieldValue = resolvers[fieldName](\n        output,\n        fieldArgs || ({} as Variables),\n        store,\n        ctx\n      );\n\n      if (node.selectionSet) {\n        // When it has a selection set we are resolving an entity with a\n        // subselection. This can either be a list or an object.\n        dataFieldValue = resolveResolverResult(\n          ctx,\n          typename,\n          fieldName,\n          key,\n          getSelectionSet(node),\n          (output[fieldAlias] !== undefined\n            ? output[fieldAlias]\n            : input[fieldAlias]) as Data,\n          dataFieldValue,\n          ownsData(input)\n        );\n      }\n\n      if (\n        store.schema &&\n        dataFieldValue === null &&\n        !isFieldNullable(store.schema, typename, fieldName)\n      ) {\n        // Special case for when null is not a valid value for the\n        // current field\n        return undefined;\n      }\n    } else if (!node.selectionSet) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        (output[fieldAlias] !== undefined\n          ? output[fieldAlias]\n          : input[fieldAlias]) as Data,\n        resultValue,\n        ownsData(input)\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          (output[fieldAlias] !== undefined\n            ? output[fieldAlias]\n            : input[fieldAlias]) as Data,\n          ownsData(input)\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (dataFieldValue === undefined && deferRef.current) {\n      // The field is undelivered and uncached, but is included in a deferred fragment\n      hasFields = true;\n    } else if (\n      dataFieldValue === undefined &&\n      ((store.schema && isFieldNullable(store.schema, typename, fieldName)) ||\n        !!getFieldError(ctx))\n    ) {\n      // The field is uncached or has errored, so it'll be set to null and skipped\n      hasPartials = true;\n      dataFieldValue = null;\n    } else if (dataFieldValue === undefined) {\n      // If the field isn't deferred or partial then we have to abort\n      ctx.__internal.path.pop();\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = hasFields || fieldName !== '__typename';\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n    // Check for any referential changes in the field's value\n    hasChanged = hasChanged || dataFieldValue !== input[fieldAlias];\n    if (dataFieldValue !== undefined) output[fieldAlias] = dataFieldValue;\n  }\n\n  ctx.partial = ctx.partial || hasPartials;\n  return isQuery && hasPartials && !hasFields\n    ? undefined\n    : hasChanged\n    ? output\n    : input;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  result: void | DataField,\n  skipNull: boolean\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { store } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const _isListNullable = store.schema\n      ? isListNullable(store.schema, typename, fieldName)\n      : false;\n    const data = new Array(result.length);\n    let hasChanged =\n      !Array.isArray(prevData) || result.length !== prevData.length;\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        prevData != null ? prevData[i] : undefined,\n        result[i],\n        skipNull\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childResult === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        ctx.partial =\n          ctx.partial || (childResult === undefined && _isListNullable);\n        data[i] = childResult != null ? childResult : null;\n        hasChanged = hasChanged || data[i] !== prevData![i];\n      }\n    }\n\n    return hasChanged ? data : prevData;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (skipNull && prevData === null) {\n    return null;\n  } else if (isDataOrKey(result)) {\n    const data = (prevData || makeData()) as Data;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readSelection(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  skipNull: boolean\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { store } = ctx;\n    const _isListNullable = store.schema\n      ? isListNullable(store.schema, typename, fieldName)\n      : false;\n    const newLink = new Array(link.length);\n    let hasChanged =\n      !Array.isArray(prevData) || newLink.length !== prevData.length;\n    for (let i = 0, l = link.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the link\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData != null ? prevData[i] : undefined,\n        skipNull\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childLink === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        ctx.partial =\n          ctx.partial || (childLink === undefined && _isListNullable);\n        newLink[i] = childLink || null;\n        hasChanged = hasChanged || newLink[i] !== prevData![i];\n      }\n    }\n\n    return hasChanged ? newLink : (prevData as Data[]);\n  } else if (link === null || (prevData === null && skipNull)) {\n    return null;\n  }\n\n  return readSelection(ctx, link, select, (prevData || makeData()) as Data);\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import {\n  Exchange,\n  formatDocument,\n  makeOperation,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from '@urql/core';\n\nimport {\n  filter,\n  map,\n  merge,\n  pipe,\n  share,\n  fromPromise,\n  fromArray,\n  mergeMap,\n  empty,\n  Source,\n  skipUntil,\n  buffer,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { makeDict, isDictEmpty } from './helpers/dict';\nimport { addCacheOutcome, toRequestPolicy } from './helpers/operation';\nimport { filterVariables, getMainOperation } from './ast';\nimport { Store, noopDataState, hydrateData, reserveLayer } from './store';\nimport { Data, Dependencies, CacheExchangeOpts } from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n  dependencies: Dependencies;\n};\n\ntype Operations = Set<number>;\ntype OperationMap = Map<number, Operation>;\ntype ResultMap = Map<number, Data | null>;\ntype OptimisticDependencies = Map<number, Dependencies>;\ntype DependentOperations = Record<string, number[]>;\n\nexport const cacheExchange = <C extends Partial<CacheExchangeOpts>>(\n  opts?: C\n): Exchange => ({ forward, client, dispatchDebug }) => {\n  const store = new Store<C>(opts);\n\n  let hydration: void | Promise<void>;\n  if (opts && opts.storage) {\n    hydration = opts.storage.readData().then(entries => {\n      hydrateData(store.data, opts!.storage!, entries);\n    });\n  }\n\n  const optimisticKeysToDependencies: OptimisticDependencies = new Map();\n  const mutationResultBuffer: OperationResult[] = [];\n  const operations: OperationMap = new Map();\n  const results: ResultMap = new Map();\n  const blockedDependencies: Dependencies = makeDict();\n  const requestedRefetch: Operations = new Set();\n  const deps: DependentOperations = makeDict();\n\n  const isBlockedByOptimisticUpdate = (dependencies: Dependencies): boolean => {\n    for (const dep in dependencies) if (blockedDependencies[dep]) return true;\n    return false;\n  };\n\n  const collectPendingOperations = (\n    pendingOperations: Operations,\n    dependencies: void | Dependencies\n  ) => {\n    if (dependencies) {\n      // Collect operations that will be updated due to cache changes\n      for (const dep in dependencies) {\n        const keys = deps[dep];\n        if (keys) {\n          deps[dep] = [];\n          for (let i = 0, l = keys.length; i < l; i++) {\n            pendingOperations.add(keys[i]);\n          }\n        }\n      }\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Operations\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    pendingOperations.forEach(key => {\n      if (key !== operation.key) {\n        const op = operations.get(key);\n        if (op) {\n          operations.delete(key);\n          let policy: RequestPolicy = 'cache-first';\n          if (requestedRefetch.has(key)) {\n            requestedRefetch.delete(key);\n            policy = 'cache-and-network';\n          }\n          client.reexecuteOperation(toRequestPolicy(op, policy));\n        }\n      }\n    });\n  };\n\n  // This registers queries with the data layer to ensure commutativity\n  const prepareForwardedOperation = (operation: Operation) => {\n    if (operation.kind === 'query') {\n      // Pre-reserve the position of the result layer\n      reserveLayer(store.data, operation.key);\n    } else if (operation.kind === 'teardown') {\n      // Delete reference to operation if any exists to release it\n      operations.delete(operation.key);\n      results.delete(operation.key);\n      // Mark operation layer as done\n      noopDataState(store.data, operation.key);\n    } else if (\n      operation.kind === 'mutation' &&\n      operation.context.requestPolicy !== 'network-only'\n    ) {\n      // This executes an optimistic update for mutations and registers it if necessary\n      const { dependencies } = writeOptimistic(store, operation, operation.key);\n      if (!isDictEmpty(dependencies)) {\n        // Update blocked optimistic dependencies\n        for (const dep in dependencies) {\n          blockedDependencies[dep] = true;\n        }\n\n        // Store optimistic dependencies for update\n        optimisticKeysToDependencies.set(operation.key, dependencies);\n\n        // Update related queries\n        const pendingOperations: Operations = new Set();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n\n    return makeOperation(\n      operation.kind,\n      {\n        key: operation.key,\n        query: formatDocument(operation.query),\n        variables: operation.variables\n          ? filterVariables(\n              getMainOperation(operation.query),\n              operation.variables\n            )\n          : operation.variables,\n      },\n      operation.context\n    );\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Dependencies) => {\n    for (const dep in dependencies) {\n      (deps[dep] || (deps[dep] = [])).push(op.key);\n      operations.set(op.key, op);\n    }\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const result = query(store, operation, results.get(operation.key));\n    const cacheOutcome: CacheOutcome = result.data\n      ? !result.partial\n        ? 'hit'\n        : 'partial'\n      : 'miss';\n\n    results.set(operation.key, result.data);\n    updateDependencies(operation, result.dependencies);\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data: result.data,\n      dependencies: result.dependencies,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (\n    result: OperationResult,\n    pendingOperations: Operations\n  ): OperationResult => {\n    const { operation, error, extensions } = result;\n    const { key } = operation;\n\n    if (operation.kind === 'mutation') {\n      // Collect previous dependencies that have been written for optimistic updates\n      const dependencies = optimisticKeysToDependencies.get(key);\n      collectPendingOperations(pendingOperations, dependencies);\n      optimisticKeysToDependencies.delete(key);\n    }\n\n    reserveLayer(store.data, operation.key, result.hasNext);\n\n    let queryDependencies: void | Dependencies;\n    let data: Data | null = result.data;\n    if (data) {\n      // Write the result to cache and collect all dependencies that need to be\n      // updated\n      const writeDependencies = write(store, operation, data, result.error, key)\n        .dependencies;\n      collectPendingOperations(pendingOperations, writeDependencies);\n\n      const queryResult = query(\n        store,\n        operation,\n        operation.kind === 'query' ? results.get(operation.key) || data : data,\n        result.error,\n        key\n      );\n\n      data = queryResult.data;\n      if (operation.kind === 'query') {\n        // Collect the query's dependencies for future pending operation updates\n        queryDependencies = queryResult.dependencies;\n        collectPendingOperations(pendingOperations, queryDependencies);\n        results.set(operation.key, result.data);\n      }\n    } else {\n      noopDataState(store.data, operation.key);\n    }\n\n    // Update this operation's dependencies if it's a query\n    if (queryDependencies) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Buffer operations while waiting on hydration to finish\n    // If no hydration takes place we replace this stream with an empty one\n    const inputOps$ = hydration\n      ? share(\n          merge([\n            pipe(\n              sharedOps$,\n              buffer(fromPromise(hydration)),\n              mergeMap(fromArray)\n            ),\n            pipe(sharedOps$, skipUntil(fromPromise(hydration))),\n          ])\n        )\n      : sharedOps$;\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cacheOps$ = pipe(\n      inputOps$,\n      filter(op => {\n        return (\n          op.kind === 'query' && op.context.requestPolicy !== 'network-only'\n        );\n      }),\n      map(operationResultFromCache),\n      share\n    );\n\n    const nonCacheOps$ = pipe(\n      inputOps$,\n      filter(op => {\n        return (\n          op.kind !== 'query' || op.context.requestPolicy === 'network-only'\n        );\n      })\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheMissOps$ = pipe(\n      cacheOps$,\n      filter(res => {\n        return (\n          res.outcome === 'miss' &&\n          res.operation.context.requestPolicy !== 'cache-only' &&\n          !isBlockedByOptimisticUpdate(res.dependencies)\n        );\n      }),\n      map(res => {\n        dispatchDebug({\n          type: 'cacheMiss',\n          message: 'The result could not be retrieved from the cache',\n          operation: res.operation,\n        });\n        return addCacheOutcome(res.operation, 'miss');\n      })\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cacheOps$,\n      filter(\n        res =>\n          res.outcome !== 'miss' ||\n          res.operation.context.requestPolicy === 'cache-only'\n      ),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome, dependencies } = res;\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            operation.context.requestPolicy === 'cache-and-network' ||\n            (operation.context.requestPolicy === 'cache-first' &&\n              outcome === 'partial')\n          ) {\n            result.stale = true;\n            if (!isBlockedByOptimisticUpdate(dependencies)) {\n              client.reexecuteOperation(\n                toRequestPolicy(operation, 'network-only')\n              );\n            } else if (\n              operation.context.requestPolicy === 'cache-and-network'\n            ) {\n              requestedRefetch.add(operation.key);\n            }\n          }\n\n          dispatchDebug({\n            type: 'cacheHit',\n            message: `A requested operation was found and returned from the cache.`,\n            operation: res.operation,\n            data: {\n              value: result,\n            },\n          });\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      merge([nonCacheOps$, cacheMissOps$]),\n      map(prepareForwardedOperation),\n      forward,\n      share\n    );\n\n    // Results that can immediately be resolved\n    const nonOptimisticResults$ = pipe(\n      result$,\n      filter(result => !optimisticKeysToDependencies.has(result.operation.key)),\n      map(result => {\n        const pendingOperations: Operations = new Set();\n        // Update the cache with the incoming API result\n        const cacheResult = updateCacheWithResult(result, pendingOperations);\n        // Execute all dependent queries\n        executePendingOperations(result.operation, pendingOperations);\n        return cacheResult;\n      })\n    );\n\n    // Prevent mutations that were previously optimistic from being flushed\n    // immediately and instead clear them out slowly\n    const optimisticMutationCompletion$ = pipe(\n      result$,\n      filter(result => optimisticKeysToDependencies.has(result.operation.key)),\n      mergeMap(\n        (result: OperationResult): Source<OperationResult> => {\n          const length = mutationResultBuffer.push(result);\n          if (length < optimisticKeysToDependencies.size) {\n            return empty;\n          }\n\n          for (let i = 0; i < mutationResultBuffer.length; i++) {\n            reserveLayer(store.data, mutationResultBuffer[i].operation.key);\n          }\n\n          for (const dep in blockedDependencies) {\n            delete blockedDependencies[dep];\n          }\n\n          const results: OperationResult[] = [];\n          const pendingOperations: Operations = new Set();\n\n          let bufferedResult: OperationResult | void;\n          while ((bufferedResult = mutationResultBuffer.shift()))\n            results.push(\n              updateCacheWithResult(bufferedResult, pendingOperations)\n            );\n\n          // Execute all dependent queries as a single batch\n          executePendingOperations(result.operation, pendingOperations);\n\n          return fromArray(results);\n        }\n      )\n    );\n\n    return merge([\n      nonOptimisticResults$,\n      optimisticMutationCompletion$,\n      cacheResult$,\n    ]);\n  };\n};\n","import {\n  Operation,\n  RequestPolicy,\n  CacheOutcome,\n  makeOperation,\n} from '@urql/core';\n\n// Returns the given operation result with added cacheOutcome meta field\nexport const addCacheOutcome = (\n  operation: Operation,\n  outcome: CacheOutcome\n): Operation =>\n  makeOperation(operation.kind, operation, {\n    ...operation.context,\n    meta: {\n      ...operation.context.meta,\n      cacheOutcome: outcome,\n    },\n  });\n\n// Copy an operation and change the requestPolicy to skip the cache\nexport const toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => {\n  return makeOperation(operation.kind, operation, {\n    ...operation.context,\n    requestPolicy,\n  });\n};\n","import { pipe, merge, makeSubject, share, filter } from 'wonka';\nimport { print, SelectionNode } from 'graphql';\n\nimport {\n  Operation,\n  Exchange,\n  ExchangeIO,\n  CombinedError,\n  createRequest,\n  makeOperation,\n} from '@urql/core';\n\nimport {\n  getMainOperation,\n  getFragments,\n  isInlineFragment,\n  isFieldNode,\n  shouldInclude,\n  getSelectionSet,\n  getName,\n} from './ast';\n\nimport {\n  SerializedRequest,\n  OptimisticMutationConfig,\n  Variables,\n  CacheExchangeOpts,\n} from './types';\n\nimport { makeDict } from './helpers/dict';\nimport { cacheExchange } from './cacheExchange';\nimport { toRequestPolicy } from './helpers/operation';\n\n/** Determines whether a given query contains an optimistic mutation field */\nconst isOptimisticMutation = <T extends OptimisticMutationConfig>(\n  config: T,\n  operation: Operation\n) => {\n  const vars: Variables = operation.variables || makeDict();\n  const fragments = getFragments(operation.query);\n  const selections = [...getSelectionSet(getMainOperation(operation.query))];\n\n  let field: void | SelectionNode;\n  while ((field = selections.pop())) {\n    if (!shouldInclude(field, vars)) {\n      continue;\n    } else if (!isFieldNode(field)) {\n      const fragmentNode = !isInlineFragment(field)\n        ? fragments[getName(field)]\n        : field;\n      if (fragmentNode) selections.push(...getSelectionSet(fragmentNode));\n    } else if (config[getName(field)]) {\n      return true;\n    }\n  }\n\n  return false;\n};\n\nconst isOfflineError = (error: undefined | CombinedError) =>\n  error &&\n  error.networkError &&\n  !error.response &&\n  ((typeof navigator !== 'undefined' && navigator.onLine === false) ||\n    /request failed|failed to fetch|network\\s?error/i.test(\n      error.networkError.message\n    ));\n\nexport const offlineExchange = <C extends Partial<CacheExchangeOpts>>(\n  opts: C\n): Exchange => input => {\n  const { storage } = opts;\n\n  if (\n    storage &&\n    storage.onOnline &&\n    storage.readMetadata &&\n    storage.writeMetadata\n  ) {\n    const { forward: outerForward, client, dispatchDebug } = input;\n    const { source: reboundOps$, next } = makeSubject<Operation>();\n    const optimisticMutations = opts.optimistic || {};\n    const failedQueue: Operation[] = [];\n\n    const updateMetadata = () => {\n      const requests: SerializedRequest[] = [];\n      for (let i = 0; i < failedQueue.length; i++) {\n        const operation = failedQueue[i];\n        if (operation.kind === 'mutation') {\n          requests.push({\n            query: print(operation.query),\n            variables: operation.variables,\n          });\n        }\n      }\n      storage.writeMetadata!(requests);\n    };\n\n    let isFlushingQueue = false;\n    const flushQueue = () => {\n      if (!isFlushingQueue) {\n        isFlushingQueue = true;\n\n        for (let i = 0; i < failedQueue.length; i++) {\n          const operation = failedQueue[i];\n          if (operation.kind === 'mutation') {\n            next(makeOperation('teardown', operation));\n          }\n        }\n\n        for (let i = 0; i < failedQueue.length; i++)\n          client.reexecuteOperation(failedQueue[i]);\n\n        failedQueue.length = 0;\n        isFlushingQueue = false;\n        updateMetadata();\n      }\n    };\n\n    const forward: ExchangeIO = ops$ => {\n      return pipe(\n        outerForward(ops$),\n        filter(res => {\n          if (\n            res.operation.kind === 'mutation' &&\n            isOfflineError(res.error) &&\n            isOptimisticMutation(optimisticMutations, res.operation)\n          ) {\n            failedQueue.push(res.operation);\n            updateMetadata();\n            return false;\n          }\n\n          return true;\n        })\n      );\n    };\n\n    storage.onOnline(flushQueue);\n    storage.readMetadata().then(mutations => {\n      if (mutations) {\n        for (let i = 0; i < mutations.length; i++) {\n          failedQueue.push(\n            client.createRequestOperation(\n              'mutation',\n              createRequest(mutations[i].query, mutations[i].variables)\n            )\n          );\n        }\n\n        flushQueue();\n      }\n    });\n\n    const cacheResults$ = cacheExchange(opts)({\n      client,\n      dispatchDebug,\n      forward,\n    });\n\n    return ops$ => {\n      const sharedOps$ = share(ops$);\n      const opsAndRebound$ = merge([reboundOps$, sharedOps$]);\n\n      return pipe(\n        cacheResults$(opsAndRebound$),\n        filter(res => {\n          if (res.operation.kind === 'query' && isOfflineError(res.error)) {\n            next(toRequestPolicy(res.operation, 'cache-only'));\n            failedQueue.push(res.operation);\n            return false;\n          }\n\n          return true;\n        })\n      );\n    };\n  }\n\n  return cacheExchange(opts)(input);\n};\n"]},"metadata":{},"sourceType":"module"}